1
00:00:01,520 --> 00:00:07,980
all right hello everyone I'm today going

2
00:00:07,980 --> 00:00:10,920
to talk about this paper about how

3
00:00:10,920 --> 00:00:15,299
Facebook uses memcache I'm in order to

4
00:00:15,299 --> 00:00:19,109
handle enormous load the reason we're

5
00:00:19,109 --> 00:00:21,000
reading this paper is that it's an

6
00:00:21,000 --> 00:00:23,750
experience paper there's not really any

7
00:00:23,750 --> 00:00:26,670
new concepts or ideas or techniques here

8
00:00:26,670 --> 00:00:32,510
but it's kind of what a real live

9
00:00:32,840 --> 00:00:36,270
company ran into when they were trying

10
00:00:36,270 --> 00:00:38,460
to build very high capacity

11
00:00:38,460 --> 00:00:40,860
infrastructure there's a couple of ways

12
00:00:40,860 --> 00:00:44,160
you could read it one is a sort of

13
00:00:44,160 --> 00:00:47,550
cautionary tale about what goes wrong if

14
00:00:47,550 --> 00:00:49,800
you don't take consistency seriously

15
00:00:49,800 --> 00:00:53,219
from the start another way to read it is

16
00:00:53,219 --> 00:00:55,199
that it's an impressive story about how

17
00:00:55,199 --> 00:00:58,140
to get extremely high capacity from

18
00:00:58,140 --> 00:01:02,300
using mostly off-the-shelf software

19
00:01:03,019 --> 00:01:05,580
another way to read it is that it's a

20
00:01:05,580 --> 00:01:08,250
kind of illustration or the fundamental

21
00:01:08,250 --> 00:01:11,490
struggle that a lot of setups face

22
00:01:11,490 --> 00:01:13,470
between trying to get very high

23
00:01:13,470 --> 00:01:16,259
performance which you do by things like

24
00:01:16,259 --> 00:01:18,299
replication and how to give consistency

25
00:01:18,299 --> 00:01:20,640
for which techniques like replication

26
00:01:20,640 --> 00:01:24,180
are really the enemy and so you know we

27
00:01:24,180 --> 00:01:26,970
can argue about whether we like their

28
00:01:26,970 --> 00:01:28,770
design or we think it's elegant or a

29
00:01:28,770 --> 00:01:31,860
good solution but we can't really argue

30
00:01:31,860 --> 00:01:35,850
with how successful they've been so we

31
00:01:35,850 --> 00:01:37,829
do need to take them seriously and for

32
00:01:37,829 --> 00:01:40,020
me actually this paper which I first

33
00:01:40,020 --> 00:01:43,590
read quite a few years ago it's been I

34
00:01:43,590 --> 00:01:45,659
thought about it a lot and it's been

35
00:01:45,659 --> 00:01:51,479
sort of a source of sort of ideas and

36
00:01:51,479 --> 00:01:55,170
understanding about problems at many

37
00:01:55,170 --> 00:01:57,380
points

38
00:01:57,890 --> 00:02:01,290
all right so before talking about

39
00:02:01,290 --> 00:02:03,240
Facebook proper you know they're an

40
00:02:03,240 --> 00:02:06,240
example of a pattern that you see fairly

41
00:02:06,240 --> 00:02:07,619
often or that many people have

42
00:02:07,619 --> 00:02:09,149
experienced in which they're trying to

43
00:02:09,149 --> 00:02:11,068
build a website to do something and you

44
00:02:11,068 --> 00:02:13,020
know typically people who build websites

45
00:02:13,020 --> 00:02:15,000
are not interested in building high

46
00:02:15,000 --> 00:02:20,190
performance you know high performance

47
00:02:20,190 --> 00:02:22,200
storage infrastructure they're

48
00:02:22,200 --> 00:02:24,750
interested in building features that

49
00:02:24,750 --> 00:02:26,940
will make their users happy or selling

50
00:02:26,940 --> 00:02:29,220
more advertisements or something you

51
00:02:29,220 --> 00:02:31,290
know so they they're not gonna start by

52
00:02:31,290 --> 00:02:33,450
spending a main year of effort or a

53
00:02:33,450 --> 00:02:35,400
whole lot of time building cool

54
00:02:35,400 --> 00:02:36,450
infrastructure they're gonna start by

55
00:02:36,450 --> 00:02:38,340
building features in that they'll sort

56
00:02:38,340 --> 00:02:41,850
of make infrastructure better only to

57
00:02:41,850 --> 00:02:42,989
the extent that they really have to

58
00:02:42,989 --> 00:02:44,070
because you know that's the best use of

59
00:02:44,070 --> 00:02:47,910
their time alright so a typical starting

60
00:02:47,910 --> 00:02:51,840
scenario in a ways when a some website

61
00:02:51,840 --> 00:02:54,180
is very small is you know there's no

62
00:02:54,180 --> 00:02:55,800
point in starting with anything more

63
00:02:55,800 --> 00:02:57,569
than just a single machine right you

64
00:02:57,569 --> 00:02:59,160
know maybe you started you only have a

65
00:02:59,160 --> 00:03:01,980
couple users sitting in front of their

66
00:03:01,980 --> 00:03:05,220
browsers and you know they talk over the

67
00:03:05,220 --> 00:03:07,260
internet here with your single machine

68
00:03:07,260 --> 00:03:09,540
your single machine is gonna maybe run

69
00:03:09,540 --> 00:03:16,170
the Apache web server now maybe you

70
00:03:16,170 --> 00:03:20,450
write the scripts that produce web pages

71
00:03:20,450 --> 00:03:23,970
using PHP or Python or some other

72
00:03:23,970 --> 00:03:27,000
convenient easy to program sort of

73
00:03:27,000 --> 00:03:29,340
scripting style language and Facebook

74
00:03:29,340 --> 00:03:33,060
uses PHP you need to store your data

75
00:03:33,060 --> 00:03:36,000
somewhere or you can just download sort

76
00:03:36,000 --> 00:03:41,190
of standard database and Facebook happen

77
00:03:41,190 --> 00:03:42,630
to use my sequel my school is a good

78
00:03:42,630 --> 00:03:45,630
choice because it implements the sequel

79
00:03:45,630 --> 00:03:49,079
query language is very powerful and acid

80
00:03:49,079 --> 00:03:51,510
transactions provides durable storage so

81
00:03:51,510 --> 00:03:54,930
this is like a very very nice set up I

82
00:03:54,930 --> 00:03:58,320
am will take you a long way actually but

83
00:03:58,320 --> 00:04:00,450
supposing supposing you get successful

84
00:04:00,450 --> 00:04:02,670
you get more and more users you know

85
00:04:02,670 --> 00:04:04,350
you're gonna get more and more load more

86
00:04:04,350 --> 00:04:07,049
and more people gonna be viewing your

87
00:04:07,049 --> 00:04:09,900
website and running whatever PHP stuff

88
00:04:09,900 --> 00:04:10,590
you're with

89
00:04:10,590 --> 00:04:14,970
site provides and so at some point

90
00:04:14,970 --> 00:04:17,190
almost certainly the first thing that's

91
00:04:17,190 --> 00:04:19,290
going to go wrong is that the PHP

92
00:04:19,290 --> 00:04:21,839
scripts are gonna take up too much CPU

93
00:04:21,839 --> 00:04:25,650
time that's usually the first bottleneck

94
00:04:25,650 --> 00:04:26,880
people encounter if they start with a

95
00:04:26,880 --> 00:04:29,850
single server so what you need is some

96
00:04:29,850 --> 00:04:31,919
way to get more horsepower for your PHP

97
00:04:31,919 --> 00:04:33,840
scripts and so that takes us to kind of

98
00:04:33,840 --> 00:04:38,430
architecture number two for websites in

99
00:04:38,430 --> 00:04:40,470
which you know you have lots and lots of

100
00:04:40,470 --> 00:04:44,450
users right or more users than before

101
00:04:44,450 --> 00:04:47,400
you need more CPU power for your PHP

102
00:04:47,400 --> 00:04:50,040
scripts so you run a bunch of front end

103
00:04:50,040 --> 00:04:53,520
servers whose only job is to run the web

104
00:04:53,520 --> 00:04:57,139
servers that users browsers talk to and

105
00:04:57,139 --> 00:05:00,479
these are called front end servers so

106
00:05:00,479 --> 00:05:03,860
these are going to run a patch either

107
00:05:04,650 --> 00:05:09,240
webserver and the PHP scripts now you

108
00:05:09,240 --> 00:05:10,560
know you users are going to talk to

109
00:05:10,560 --> 00:05:11,880
different servers at different times

110
00:05:11,880 --> 00:05:14,610
maybe your users Quadra each other they

111
00:05:14,610 --> 00:05:16,259
message each other they need to see each

112
00:05:16,259 --> 00:05:17,460
other's posts or something

113
00:05:17,460 --> 00:05:19,080
so all these front-end servers are going

114
00:05:19,080 --> 00:05:22,699
to need to see the same back-end data

115
00:05:23,210 --> 00:05:27,120
and in order to do that you probably

116
00:05:27,120 --> 00:05:28,889
can't just stick at least for a while

117
00:05:28,889 --> 00:05:30,300
you can just stick with one database

118
00:05:30,300 --> 00:05:31,500
server so you gonna have a single

119
00:05:31,500 --> 00:05:35,310
machine already my sequel that handles

120
00:05:35,310 --> 00:05:37,770
all of the database all queries and

121
00:05:37,770 --> 00:05:40,530
updates reads and writes from the front

122
00:05:40,530 --> 00:05:43,199
end servers and if you possibly can it's

123
00:05:43,199 --> 00:05:45,060
wise to use a single server here because

124
00:05:45,060 --> 00:05:46,560
as soon as you go with two servers and

125
00:05:46,560 --> 00:05:49,050
somehow your data over multiple database

126
00:05:49,050 --> 00:05:51,620
servers like gets much more complicated

127
00:05:51,620 --> 00:05:54,060
and you need to worry about things like

128
00:05:54,060 --> 00:05:56,550
do you need distributed transactions or

129
00:05:56,550 --> 00:05:58,830
how it has the PHP scripts decide which

130
00:05:58,830 --> 00:06:01,590
database server to talk to and so again

131
00:06:01,590 --> 00:06:03,389
you can get a long way with this second

132
00:06:03,389 --> 00:06:05,699
architecture you have as much CPU power

133
00:06:05,699 --> 00:06:07,889
as you like by adding more front end

134
00:06:07,889 --> 00:06:11,880
servers and up to a point a single

135
00:06:11,880 --> 00:06:13,590
database server will actually be able to

136
00:06:13,590 --> 00:06:16,020
absorb the reason rights of many front

137
00:06:16,020 --> 00:06:18,000
ends but you know maybe you're very

138
00:06:18,000 --> 00:06:22,610
successful you get even more users and

139
00:06:22,610 --> 00:06:24,840
so the question is what's gonna go wrong

140
00:06:24,840 --> 00:06:27,599
next and typically what goes wrong next

141
00:06:27,599 --> 00:06:30,090
is that the database server since you

142
00:06:30,090 --> 00:06:33,169
can always add more CPU more web servers

143
00:06:33,169 --> 00:06:35,430
you know what inevitably goes wrong is

144
00:06:35,430 --> 00:06:37,620
that after a while the database server

145
00:06:37,620 --> 00:06:43,380
runs out of steam okay so what's the

146
00:06:43,380 --> 00:06:49,380
next architecture this is web

147
00:06:49,380 --> 00:06:51,690
architecture 3 and the kind of standard

148
00:06:51,690 --> 00:06:55,080
evolution of big websites here we have

149
00:06:55,080 --> 00:06:57,930
the same if you know now thousands and

150
00:06:57,930 --> 00:07:01,500
thousands of users lots and lots of

151
00:07:01,500 --> 00:07:04,740
front ends and now we basically we know

152
00:07:04,740 --> 00:07:07,909
we're gonna have to have multiple

153
00:07:07,909 --> 00:07:10,590
database servers so now behind the front

154
00:07:10,590 --> 00:07:14,159
ends we have a whole rack of database

155
00:07:14,159 --> 00:07:16,860
servers each one of them running my

156
00:07:16,860 --> 00:07:18,280
sequel again

157
00:07:18,280 --> 00:07:21,220
but we're going to shard the data

158
00:07:21,220 --> 00:07:22,960
we're driven now to sharding the data

159
00:07:22,960 --> 00:07:25,390
over the database server so you know

160
00:07:25,390 --> 00:07:27,130
maybe the first one holds keys you know

161
00:07:27,130 --> 00:07:32,080
a through G & G through second one holds

162
00:07:32,080 --> 00:07:34,660
keys G through Q and you know whatever

163
00:07:34,660 --> 00:07:37,360
the charting happens to be and now the

164
00:07:37,360 --> 00:07:38,680
front-end you know you have to teach

165
00:07:38,680 --> 00:07:40,960
your PHP scripts here to look at the

166
00:07:40,960 --> 00:07:42,250
data they need and try to figure out

167
00:07:42,250 --> 00:07:43,510
which database server they're going to

168
00:07:43,510 --> 00:07:45,100
talk to it you know in different times

169
00:07:45,100 --> 00:07:46,360
for different data they're going to talk

170
00:07:46,360 --> 00:07:50,250
to different servers so this is sharding

171
00:07:51,030 --> 00:07:53,230
and of course the reason why this gives

172
00:07:53,230 --> 00:07:57,460
you a boost is that now the all the work

173
00:07:57,460 --> 00:07:58,720
of reading and writing has split up

174
00:07:58,720 --> 00:08:01,360
hopefully hopefully evenly split up

175
00:08:01,360 --> 00:08:04,090
between these servers since they hold

176
00:08:04,090 --> 00:08:05,830
different data now replicas rating word

177
00:08:05,830 --> 00:08:08,170
charting the data and they can execute

178
00:08:08,170 --> 00:08:09,960
in parallel and have big parallel

179
00:08:09,960 --> 00:08:14,980
capacity to read and write data it's a

180
00:08:14,980 --> 00:08:17,110
little bit painful the PHP code has to

181
00:08:17,110 --> 00:08:19,180
know about the sharding if you change

182
00:08:19,180 --> 00:08:21,460
the setup of the database servers that

183
00:08:21,460 --> 00:08:23,440
you add a new database server or you

184
00:08:23,440 --> 00:08:25,330
realize you need to split up the keys

185
00:08:25,330 --> 00:08:27,430
differently you know now you need a

186
00:08:27,430 --> 00:08:29,320
you're gonna have to modify the software

187
00:08:29,320 --> 00:08:31,360
running on the front ends or something

188
00:08:31,360 --> 00:08:33,370
in order for them to understand about

189
00:08:33,370 --> 00:08:35,620
how to cut over to the new sharding so

190
00:08:35,620 --> 00:08:37,750
there's some there's some pain here

191
00:08:37,750 --> 00:08:40,030
there's also if you need transactions

192
00:08:40,030 --> 00:08:42,760
and you know many people use them if you

193
00:08:42,760 --> 00:08:45,130
need transactions but the data involved

194
00:08:45,130 --> 00:08:47,140
in a single transaction is on more than

195
00:08:47,140 --> 00:08:49,870
one database server you're probably

196
00:08:49,870 --> 00:08:51,730
going to need two-phase commit or some

197
00:08:51,730 --> 00:08:53,610
other distributed transaction scheme

198
00:08:53,610 --> 00:08:59,040
it's also a pain and slow all right well

199
00:08:59,040 --> 00:09:03,490
you can you can get fairly far with this

200
00:09:03,490 --> 00:09:07,089
arrangement however it's quite expensive

201
00:09:07,089 --> 00:09:10,750
my sequel or sort of you know fully

202
00:09:10,750 --> 00:09:12,520
featured database servers like people

203
00:09:12,520 --> 00:09:14,680
like to use it's not particularly fast

204
00:09:14,680 --> 00:09:18,130
it can probably

205
00:09:18,130 --> 00:09:20,380
perform a couple hundred thousand reads

206
00:09:20,380 --> 00:09:24,340
per second and far fewer rights and you

207
00:09:24,340 --> 00:09:29,790
know web sites tend to be read heavy so

208
00:09:29,790 --> 00:09:32,200
it's likely that you're gonna run out of

209
00:09:32,200 --> 00:09:36,300
steam for reads before writes that

210
00:09:36,300 --> 00:09:38,410
traffic will be that we load on the web

211
00:09:38,410 --> 00:09:40,150
servers will be dominated by reads and

212
00:09:40,150 --> 00:09:43,660
so after a while you know you can slice

213
00:09:43,660 --> 00:09:45,790
the data more and more thinly over more

214
00:09:45,790 --> 00:09:49,210
and more servers but two things go wrong

215
00:09:49,210 --> 00:09:51,910
with that one is that the some sometimes

216
00:09:51,910 --> 00:09:54,430
you're you have specific keys that are

217
00:09:54,430 --> 00:09:57,340
hot that are used a lot and no amount of

218
00:09:57,340 --> 00:09:59,080
slicing really helps there because each

219
00:09:59,080 --> 00:10:02,110
key is only on a single server so that

220
00:10:02,110 --> 00:10:03,550
keeps very popular that servers can be

221
00:10:03,550 --> 00:10:04,990
overloaded no matter how much you

222
00:10:04,990 --> 00:10:09,160
partition or shard the data and the

223
00:10:09,160 --> 00:10:10,980
other problem with adding was shorting

224
00:10:10,980 --> 00:10:13,060
adding lots and lots of my sequel

225
00:10:13,060 --> 00:10:17,010
database servers for sharding is that

226
00:10:17,010 --> 00:10:20,080
it's really an expensive way to go as it

227
00:10:20,080 --> 00:10:21,640
turns out and after a point you're gonna

228
00:10:21,640 --> 00:10:23,440
you're going to start to think that well

229
00:10:23,440 --> 00:10:25,060
instead of spending a lot of money to

230
00:10:25,060 --> 00:10:27,760
add another database server running my

231
00:10:27,760 --> 00:10:30,880
sequel I could take the same server run

232
00:10:30,880 --> 00:10:32,920
something much faster on it like as it

233
00:10:32,920 --> 00:10:35,470
happens memcache D and get a lot more

234
00:10:35,470 --> 00:10:37,420
reads per second out of the same

235
00:10:37,420 --> 00:10:40,090
Hardware using caching than using

236
00:10:40,090 --> 00:10:45,640
databases so the next architecture and

237
00:10:45,640 --> 00:10:48,280
this is now starting to resemble what

238
00:10:48,280 --> 00:10:51,940
Facebook is using the next architecture

239
00:10:51,940 --> 00:10:56,680
still need users we still have a bunch

240
00:10:56,680 --> 00:10:59,950
of front end servers running web servers

241
00:10:59,950 --> 00:11:03,460
in PHP and by now maybe a vast number of

242
00:11:03,460 --> 00:11:05,110
front end servers we still have our

243
00:11:05,110 --> 00:11:06,640
database servers because you know we

244
00:11:06,640 --> 00:11:11,020
need us a system that will store data

245
00:11:11,020 --> 00:11:13,540
safely on disk for us and we'll provide

246
00:11:13,540 --> 00:11:17,860
things like transactions for us and so

247
00:11:17,860 --> 00:11:19,300
you know probably want a database for

248
00:11:19,300 --> 00:11:21,940
that but in between we're gonna have a

249
00:11:21,940 --> 00:11:23,710
caching layer that's this is where

250
00:11:23,710 --> 00:11:26,160
memcache D comes in

251
00:11:26,160 --> 00:11:27,959
and of course there's other things you

252
00:11:27,959 --> 00:11:29,940
could use that the memcache but memcache

253
00:11:29,940 --> 00:11:31,649
D happens to be an extremely popular

254
00:11:31,649 --> 00:11:34,230
caching scheme the idea now is you have

255
00:11:34,230 --> 00:11:36,959
a whole bunch of these memcache servers

256
00:11:36,959 --> 00:11:41,759
and when a front-end needs to read some

257
00:11:41,759 --> 00:11:45,990
data the first thing it does is ask one

258
00:11:45,990 --> 00:11:47,850
of the memcache servers look do you have

259
00:11:47,850 --> 00:11:48,839
the data I need

260
00:11:48,839 --> 00:11:50,819
so it'll send a get request with some

261
00:11:50,819 --> 00:11:54,810
key to one of the memcache servers and

262
00:11:54,810 --> 00:11:56,279
the memcache server will check it's got

263
00:11:56,279 --> 00:11:58,139
just a table in memory it's in fact

264
00:11:58,139 --> 00:12:00,689
memcache is extremely simple it's far

265
00:12:00,689 --> 00:12:04,889
far simpler than your lab 3 for example

266
00:12:04,889 --> 00:12:07,529
it just has just as a big hash table on

267
00:12:07,529 --> 00:12:09,000
memory it checks with that keys in the

268
00:12:09,000 --> 00:12:11,040
hash table if it is it sends back the

269
00:12:11,040 --> 00:12:12,509
data saying oh yeah here's the value

270
00:12:12,509 --> 00:12:15,180
I've cashed for that and if we if the

271
00:12:15,180 --> 00:12:16,470
front end hits in this memcache server

272
00:12:16,470 --> 00:12:19,319
great I can then produce the webpage

273
00:12:19,319 --> 00:12:21,389
with that data in it if it misses in the

274
00:12:21,389 --> 00:12:23,839
webserver though the front-end has to

275
00:12:23,839 --> 00:12:27,569
then rear equesticle irrelevant database

276
00:12:27,569 --> 00:12:30,689
server and the database server will say

277
00:12:30,689 --> 00:12:33,029
oh you know here's the here's the data

278
00:12:33,029 --> 00:12:36,779
you need and at that point in order to

279
00:12:36,779 --> 00:12:39,000
cash it in for the next front-end that

280
00:12:39,000 --> 00:12:42,540
needs it the front end we'll send a put

281
00:12:42,540 --> 00:12:44,670
with the data it fashion the database

282
00:12:44,670 --> 00:12:48,300
into that memcache server and because

283
00:12:48,300 --> 00:12:50,850
memcache runs at least 10 and maybe

284
00:12:50,850 --> 00:12:52,740
maybe more than 10 times faster for

285
00:12:52,740 --> 00:12:55,800
weeds than the database for a given

286
00:12:55,800 --> 00:12:58,079
amount of hardware it really pays off to

287
00:12:58,079 --> 00:12:59,819
use a fair amount some of that hardware

288
00:12:59,819 --> 00:13:02,189
for memcache as well as for the database

289
00:13:02,189 --> 00:13:05,040
servers so people people use this

290
00:13:05,040 --> 00:13:06,420
arrangement a lot and it just saves them

291
00:13:06,420 --> 00:13:09,209
money because memcache is so much faster

292
00:13:09,209 --> 00:13:12,029
for weeds than a database server still

293
00:13:12,029 --> 00:13:13,410
need to send writes to the database

294
00:13:13,410 --> 00:13:15,990
because you want right to an updates to

295
00:13:15,990 --> 00:13:20,639
be stored durably on the database as

296
00:13:20,639 --> 00:13:22,379
this can still be there if there's a

297
00:13:22,379 --> 00:13:26,309
crash or something but you can send the

298
00:13:26,309 --> 00:13:28,019
Reese to the cache very much more

299
00:13:28,019 --> 00:13:30,600
quickly ok so we have a question the

300
00:13:30,600 --> 00:13:32,639
question is why wouldn't the memcache

301
00:13:32,639 --> 00:13:34,589
server actually hit the put on behalf of

302
00:13:34,589 --> 00:13:36,329
the front-end and cache the response

303
00:13:36,329 --> 00:13:37,949
before responding the front-end so

304
00:13:37,949 --> 00:13:39,209
that's a great question

305
00:13:39,209 --> 00:13:41,100
you could imagine a caching layer that

306
00:13:41,100 --> 00:13:43,439
you would send a get to it and it would

307
00:13:43,439 --> 00:13:45,449
if it missed the memcache layer would

308
00:13:45,449 --> 00:13:48,569
would forward the request to the

309
00:13:48,569 --> 00:13:50,670
database babies respond the memcache

310
00:13:50,670 --> 00:13:52,699
memcache would add the data to its

311
00:13:52,699 --> 00:13:55,680
tables and then respond and the reason

312
00:13:55,680 --> 00:13:59,339
for this is that memcache is like a

313
00:13:59,339 --> 00:14:00,720
completely separate piece of software

314
00:14:00,720 --> 00:14:02,880
that it doesn't know anything about

315
00:14:02,880 --> 00:14:04,589
databases and it's actually not even

316
00:14:04,589 --> 00:14:06,509
necessarily used and combined in

317
00:14:06,509 --> 00:14:08,819
conjunction with the database although

318
00:14:08,819 --> 00:14:12,240
it often is so we can't bake in

319
00:14:12,240 --> 00:14:14,720
knowledge of the database into memcache

320
00:14:14,720 --> 00:14:18,480
and sort of deeper reason is that the

321
00:14:18,480 --> 00:14:22,380
front ends are often not really storing

322
00:14:22,380 --> 00:14:25,160
one for one database records in memcache

323
00:14:25,160 --> 00:14:28,980
almost always or very frequently what's

324
00:14:28,980 --> 00:14:30,899
going on is that the front-end will

325
00:14:30,899 --> 00:14:32,790
issue some requests to the database and

326
00:14:32,790 --> 00:14:35,189
then process the results somewhat you

327
00:14:35,189 --> 00:14:37,500
know maybe take a few steps to turning

328
00:14:37,500 --> 00:14:42,949
it into HTML or sort of collect together

329
00:14:42,949 --> 00:14:46,529
you know results from multiple careers

330
00:14:46,529 --> 00:14:47,910
on multiple rows in the database and

331
00:14:47,910 --> 00:14:49,860
cached partially processed information

332
00:14:49,860 --> 00:14:51,120
in memcache

333
00:14:51,120 --> 00:14:53,730
just to save the next reader from having

334
00:14:53,730 --> 00:14:56,279
to do the same processing and for that

335
00:14:56,279 --> 00:14:59,819
reason memcache it doesn't really does

336
00:14:59,819 --> 00:15:01,410
not understand the relationship between

337
00:15:01,410 --> 00:15:03,630
what the friends would like to see see

338
00:15:03,630 --> 00:15:06,089
cached and how did you ride that data

339
00:15:06,089 --> 00:15:07,350
from the database that knowledge is

340
00:15:07,350 --> 00:15:09,480
really only in the PHP code on the front

341
00:15:09,480 --> 00:15:12,449
end so therefore even though we could be

342
00:15:12,449 --> 00:15:14,850
architectural a good idea we can't have

343
00:15:14,850 --> 00:15:17,929
this integration here sort of direct

344
00:15:17,929 --> 00:15:20,279
contact between memcache and the

345
00:15:20,279 --> 00:15:22,829
database although it might make the

346
00:15:22,829 --> 00:15:25,380
cache consistency story much more

347
00:15:25,380 --> 00:15:27,870
straightforward and yes

348
00:15:27,870 --> 00:15:31,440
this is it's this and answer the next

349
00:15:31,440 --> 00:15:33,630
question that is the difference between

350
00:15:33,630 --> 00:15:37,100
a lookaside cash and a look through cash

351
00:15:37,100 --> 00:15:40,020
the fact the lookaside business is that

352
00:15:40,020 --> 00:15:42,150
the front end sort of looks asides to

353
00:15:42,150 --> 00:15:44,280
the cash to see if the data is there and

354
00:15:44,280 --> 00:15:45,930
if it's not it makes its own

355
00:15:45,930 --> 00:15:48,090
arrangements for getting the deed on

356
00:15:48,090 --> 00:15:51,000
amiss you know a look through cash my

357
00:15:51,000 --> 00:15:53,160
forward request of the database and

358
00:15:53,160 --> 00:15:57,270
directly and handle the response now

359
00:15:57,270 --> 00:15:59,700
part of the reason for the popularity in

360
00:15:59,700 --> 00:16:01,770
memcache is that it is it is a lookaside

361
00:16:01,770 --> 00:16:04,890
cash that is completely neutral about

362
00:16:04,890 --> 00:16:07,170
whether there's a database or what's in

363
00:16:07,170 --> 00:16:09,140
the database or the relationship between

364
00:16:09,140 --> 00:16:11,520
stuff in memcache and what's in the end

365
00:16:11,520 --> 00:16:17,160
items in the database all right so this

366
00:16:17,160 --> 00:16:18,840
is very popular arrangement very widely

367
00:16:18,840 --> 00:16:21,540
used it's cost effective because

368
00:16:21,540 --> 00:16:23,310
memcache is so much faster in the

369
00:16:23,310 --> 00:16:27,180
database it's a bit complex every

370
00:16:27,180 --> 00:16:29,370
website that makes serious you so this

371
00:16:29,370 --> 00:16:33,150
faces the problem that if you don't do

372
00:16:33,150 --> 00:16:35,190
something the data that's stored in the

373
00:16:35,190 --> 00:16:37,530
caches will get out of sync with the

374
00:16:37,530 --> 00:16:40,050
data in the database and so everybody

375
00:16:40,050 --> 00:16:42,600
has to have a story for how to make sure

376
00:16:42,600 --> 00:16:43,800
that when you modify something in the

377
00:16:43,800 --> 00:16:48,710
database you do something to memcache to

378
00:16:48,710 --> 00:16:50,640
you know take care of the fact that

379
00:16:50,640 --> 00:16:52,560
memcache may then be storing stale data

380
00:16:52,560 --> 00:16:54,750
that doesn't reflect the updates and a

381
00:16:54,750 --> 00:16:56,790
lot of this papers about what Facebook

382
00:16:56,790 --> 00:16:58,620
story is for that although other people

383
00:16:58,620 --> 00:17:05,030
had other plans this arrangements also

384
00:17:05,030 --> 00:17:08,040
potentially a bit fragile it allows you

385
00:17:08,040 --> 00:17:11,970
to scale up to far more users then you

386
00:17:11,970 --> 00:17:13,440
could have gone with databases alone

387
00:17:13,440 --> 00:17:15,660
because memcache is so fast but what

388
00:17:15,660 --> 00:17:17,700
that means is that you're gonna end up

389
00:17:17,700 --> 00:17:20,180
with the system that's sustaining a load

390
00:17:20,180 --> 00:17:24,060
that's far far higher you know orders of

391
00:17:24,060 --> 00:17:25,829
magnitude higher than what the databases

392
00:17:25,829 --> 00:17:29,340
could handle and thus if anything goes

393
00:17:29,340 --> 00:17:30,990
wrong for example if one of your

394
00:17:30,990 --> 00:17:34,250
memcache servers were to fail and

395
00:17:34,250 --> 00:17:36,210
meaning that the front ends would now

396
00:17:36,210 --> 00:17:37,710
have to contact the database because

397
00:17:37,710 --> 00:17:40,080
they didn't hit they couldn't use this

398
00:17:40,080 --> 00:17:41,220
to store data

399
00:17:41,220 --> 00:17:42,630
you're gonna be increasing a load in the

400
00:17:42,630 --> 00:17:45,450
databases dramatically right because

401
00:17:45,450 --> 00:17:47,120
memcache do you know supposing it has a

402
00:17:47,120 --> 00:17:50,309
you know hit rate of 99 percent or

403
00:17:50,309 --> 00:17:53,220
whatever it happens to be you know

404
00:17:53,220 --> 00:17:55,049
memcache is gonna be absorbing almost

405
00:17:55,049 --> 00:17:58,470
all the reads the database backends only

406
00:17:58,470 --> 00:18:00,450
going to be seeing a few percent of the

407
00:18:00,450 --> 00:18:04,530
total reads so any failure here is gonna

408
00:18:04,530 --> 00:18:06,179
increase that few percent of the reads

409
00:18:06,179 --> 00:18:07,950
to maybe you know I don't know 50

410
00:18:07,950 --> 00:18:09,840
percent of the reads or whatever which

411
00:18:09,840 --> 00:18:11,669
is a huge huge order of magnitude

412
00:18:11,669 --> 00:18:16,320
increase so as Facebook does once you've

413
00:18:16,320 --> 00:18:18,659
got to rely on this caching layer you

414
00:18:18,659 --> 00:18:24,299
need to be set up pretty serious

415
00:18:24,299 --> 00:18:26,429
measures to make sure that you never

416
00:18:26,429 --> 00:18:30,440
expose the database layer to the full

417
00:18:30,440 --> 00:18:35,039
anything like the full load that the

418
00:18:35,039 --> 00:18:37,289
caching layer is seeing and you know you

419
00:18:37,289 --> 00:18:40,080
see in facebook they have quite a bit of

420
00:18:40,080 --> 00:18:43,350
thought put into making sure the

421
00:18:43,350 --> 00:18:44,850
databases don't ever see anything like

422
00:18:44,850 --> 00:18:48,860
the full load okay

423
00:18:48,860 --> 00:18:54,059
so far this is generic now I want to

424
00:18:54,059 --> 00:18:58,230
sort of switch to a big picture of what

425
00:18:58,230 --> 00:19:00,600
Facebook describes in the paper for

426
00:19:00,600 --> 00:19:04,020
their overall architecture of course

427
00:19:04,020 --> 00:19:05,970
they have lots of users every user as a

428
00:19:05,970 --> 00:19:08,640
friendless and status and posts and

429
00:19:08,640 --> 00:19:13,830
likes and photos but Facebook's very

430
00:19:13,830 --> 00:19:16,650
easy or e nted towards showing data to

431
00:19:16,650 --> 00:19:21,390
users and a super important aspect of

432
00:19:21,390 --> 00:19:24,740
that is that fresh data is not

433
00:19:24,740 --> 00:19:26,909
absolutely necessary in that

434
00:19:26,909 --> 00:19:31,260
circumstance you know suppose the reads

435
00:19:31,260 --> 00:19:34,169
are you know due to caching supposed to

436
00:19:34,169 --> 00:19:36,120
reads yield data that's a few seconds

437
00:19:36,120 --> 00:19:38,429
out of date so you're showing your users

438
00:19:38,429 --> 00:19:40,110
data not the very latest data but the

439
00:19:40,110 --> 00:19:41,880
data from a few seconds ago you know

440
00:19:41,880 --> 00:19:45,000
what the users are extremely unlikely to

441
00:19:45,000 --> 00:19:48,570
notice except in special cases right if

442
00:19:48,570 --> 00:19:50,159
I'm looking at a news feed of today's

443
00:19:50,159 --> 00:19:53,309
you know today's news you know if I see

444
00:19:53,309 --> 00:19:54,419
the news from a few

445
00:19:54,419 --> 00:19:57,629
times ago versus the news from now a big

446
00:19:57,629 --> 00:20:00,210
deal nobody's gonna notice nobody's

447
00:20:00,210 --> 00:20:01,710
gonna complain you know that's not

448
00:20:01,710 --> 00:20:03,809
always true for all data but for a lot a

449
00:20:03,809 --> 00:20:05,429
lot of the data that they have to deal

450
00:20:05,429 --> 00:20:07,590
with sort of super up-to-date

451
00:20:07,590 --> 00:20:08,970
consistency in the sense of like

452
00:20:08,970 --> 00:20:11,549
linearise ability is not actually

453
00:20:11,549 --> 00:20:14,190
important what is important is that you

454
00:20:14,190 --> 00:20:17,850
don't cache stale data indefinitely you

455
00:20:17,850 --> 00:20:20,159
know what they can't do is by mistake

456
00:20:20,159 --> 00:20:22,200
have some data that they're showing

457
00:20:22,200 --> 00:20:25,259
users that's from yesterday or last week

458
00:20:25,259 --> 00:20:28,590
or even an hour ago those users really

459
00:20:28,590 --> 00:20:32,220
will start to notice that so they don't

460
00:20:32,220 --> 00:20:34,889
care about consistency like

461
00:20:34,889 --> 00:20:36,720
second-by-second but they care a lot

462
00:20:36,720 --> 00:20:41,789
about not not being in cannot chewing

463
00:20:41,789 --> 00:20:44,159
stale data from more than well more than

464
00:20:44,159 --> 00:20:47,669
a little while ago the other situation

465
00:20:47,669 --> 00:20:48,600
in which they need to provide

466
00:20:48,600 --> 00:20:51,149
consistency is if a user updates their

467
00:20:51,149 --> 00:20:54,720
own data or if a user updates almost any

468
00:20:54,720 --> 00:20:56,970
data and then reads that same data that

469
00:20:56,970 --> 00:20:59,249
the human knows that they just updated

470
00:20:59,249 --> 00:21:02,489
it's extremely confusing for the user to

471
00:21:02,489 --> 00:21:04,499
see stale data if they know they just

472
00:21:04,499 --> 00:21:07,619
changed it and so in that specific case

473
00:21:07,619 --> 00:21:11,669
the Facebook design is also careful to

474
00:21:11,669 --> 00:21:14,549
make sure that if a user changes data

475
00:21:14,549 --> 00:21:18,529
that that user will see the change data

476
00:21:19,220 --> 00:21:23,519
ok so Facebook has multiple data centers

477
00:21:23,519 --> 00:21:27,899
which they call regions and I think at

478
00:21:27,899 --> 00:21:30,389
the time this paper was written they had

479
00:21:30,389 --> 00:21:33,859
two regions their sort of primary region

480
00:21:33,859 --> 00:21:36,690
was on the west coast California and

481
00:21:36,690 --> 00:21:40,109
their sort of secondary region was in

482
00:21:40,109 --> 00:21:43,769
the East Coast and the two data centers

483
00:21:43,769 --> 00:21:51,480
look pretty similar you

484
00:21:51,480 --> 00:21:52,890
set of database servers running my

485
00:21:52,890 --> 00:21:56,520
sequel the sharted date over these my

486
00:21:56,520 --> 00:21:59,570
sequel database servers

487
00:21:59,820 --> 00:22:03,900
they had a bunch of memcache D servers

488
00:22:03,900 --> 00:22:05,710
which we'll see they are actually

489
00:22:05,710 --> 00:22:07,480
arranged in independent clusters and

490
00:22:07,480 --> 00:22:09,630
then they had a bunch of front ends

491
00:22:09,630 --> 00:22:15,160
again sort of a separate arrangement in

492
00:22:15,160 --> 00:22:20,230
each data center and there's a couple

493
00:22:20,230 --> 00:22:22,570
reasons for this one is that their

494
00:22:22,570 --> 00:22:24,370
customers were scattered all over the

495
00:22:24,370 --> 00:22:27,250
country and it's nice just for a

496
00:22:27,250 --> 00:22:28,300
performance that people on the East

497
00:22:28,300 --> 00:22:29,920
Coast can talk to a nearby data center

498
00:22:29,920 --> 00:22:31,750
and people on the west coast can also

499
00:22:31,750 --> 00:22:33,460
talk to a nearby deficit it just makes

500
00:22:33,460 --> 00:22:42,310
internet delays less now the the data

501
00:22:42,310 --> 00:22:44,500
centers were not symmetric each of them

502
00:22:44,500 --> 00:22:46,000
held a complete copy of all the data

503
00:22:46,000 --> 00:22:47,920
they didn't sort of shard the data

504
00:22:47,920 --> 00:22:50,590
across the data centers so the West

505
00:22:50,590 --> 00:22:54,040
Coast I think was a primary and it sort

506
00:22:54,040 --> 00:22:55,930
of had the real copy of the data and the

507
00:22:55,930 --> 00:23:00,370
East Coast was a secondary and what that

508
00:23:00,370 --> 00:23:01,930
really means is that all rights had to

509
00:23:01,930 --> 00:23:05,920
be sent to the relevant database and the

510
00:23:05,920 --> 00:23:08,200
primary day to be Center so you know any

511
00:23:08,200 --> 00:23:12,940
right gets sent you know here and they

512
00:23:12,940 --> 00:23:15,340
use a feature of my sequel they serve

513
00:23:15,340 --> 00:23:17,260
asynchronous log replication scheme to

514
00:23:17,260 --> 00:23:21,360
have each database in the primary region

515
00:23:21,360 --> 00:23:24,280
send every update to the corresponding

516
00:23:24,280 --> 00:23:26,380
database in secondary region so that

517
00:23:26,380 --> 00:23:29,640
with a lag of maybe even a few seconds

518
00:23:29,640 --> 00:23:31,720
these database servers would have

519
00:23:31,720 --> 00:23:33,820
identical content the secondary database

520
00:23:33,820 --> 00:23:35,710
servers would have identical content to

521
00:23:35,710 --> 00:23:38,140
the primaries reads though we're local

522
00:23:38,140 --> 00:23:39,430
so these front ends when they need to

523
00:23:39,430 --> 00:23:41,890
find some data I'm in general would talk

524
00:23:41,890 --> 00:23:44,590
to memcache memcache in that data center

525
00:23:44,590 --> 00:23:46,600
and if they missed in memcache they

526
00:23:46,600 --> 00:23:48,640
talked to the they'd read from the

527
00:23:48,640 --> 00:23:52,080
database in that same data data center

528
00:23:52,080 --> 00:23:56,880
um again though the databases are

529
00:23:56,880 --> 00:23:59,830
complete replicas all the data's on both

530
00:23:59,830 --> 00:24:03,540
of these these in both of these regions

531
00:24:04,180 --> 00:24:10,670
that's the overall picture the next

532
00:24:10,670 --> 00:24:11,990
thing I want to talk about is a few

533
00:24:11,990 --> 00:24:18,110
details about how they how they use you

534
00:24:18,110 --> 00:24:21,020
know with this leukocyte caching

535
00:24:21,020 --> 00:24:25,040
actually looks like so there's really

536
00:24:25,040 --> 00:24:28,550
there's reads and writes and this is

537
00:24:28,550 --> 00:24:31,910
just what's shown in Figure two for a

538
00:24:31,910 --> 00:24:33,640
read

539
00:24:33,640 --> 00:24:37,360
which is executing on a front-end the

540
00:24:37,360 --> 00:24:39,340
first thing if you read any data that

541
00:24:39,340 --> 00:24:40,870
might be cached the first thing that

542
00:24:40,870 --> 00:24:43,420
code in the front-end does is makes this

543
00:24:43,420 --> 00:24:46,060
get library call with the key of the

544
00:24:46,060 --> 00:24:48,370
data they want and get just generates an

545
00:24:48,370 --> 00:24:51,790
RPC to the relevant memcache server so

546
00:24:51,790 --> 00:24:56,050
they hash this library routine hashes on

547
00:24:56,050 --> 00:24:57,970
the client hashes the key to pick the

548
00:24:57,970 --> 00:25:01,420
memcache server and sends an RPC to that

549
00:25:01,420 --> 00:25:04,150
mcat server them casually reply yes

550
00:25:04,150 --> 00:25:06,670
here's your data or or maybe it'll point

551
00:25:06,670 --> 00:25:10,090
nil saying I don't have that data it's

552
00:25:10,090 --> 00:25:18,910
not cached so if if V is nil then the

553
00:25:18,910 --> 00:25:22,600
front-end will issue whatever sequel

554
00:25:22,600 --> 00:25:27,510
query is required to fetch the data from

555
00:25:27,510 --> 00:25:32,680
the database and then make another RPC

556
00:25:32,680 --> 00:25:34,920
call

557
00:25:36,980 --> 00:25:39,810
- memk2 the relevant memcache server to

558
00:25:39,810 --> 00:25:41,760
install the fetch data in the memcache

559
00:25:41,760 --> 00:25:44,070
server so this is just the routine I

560
00:25:44,070 --> 00:25:45,680
talked through before

561
00:25:45,680 --> 00:25:48,660
it's kind of what lookaside caching does

562
00:25:48,660 --> 00:25:51,710
and for right

563
00:25:55,540 --> 00:25:59,890
you know V is the writing we have a key

564
00:25:59,890 --> 00:26:01,930
and a value no Rana right and so library

565
00:26:01,930 --> 00:26:05,050
routine on an each front end we're gonna

566
00:26:05,050 --> 00:26:11,110
send the the new data to the database

567
00:26:11,110 --> 00:26:15,010
and you know I as I mentioned before the

568
00:26:15,010 --> 00:26:16,240
Keene the value may be a little bit

569
00:26:16,240 --> 00:26:17,680
different you know what's stored in the

570
00:26:17,680 --> 00:26:19,570
database is often in a somewhat

571
00:26:19,570 --> 00:26:21,300
different form from what's stored in

572
00:26:21,300 --> 00:26:23,680
memcache see but we'll imagine for now

573
00:26:23,680 --> 00:26:26,350
the same and once the database has the

574
00:26:26,350 --> 00:26:29,230
new data then the right library routine

575
00:26:29,230 --> 00:26:34,480
sends an RPC to memcache detailing it

576
00:26:34,480 --> 00:26:38,230
look you got to delete this key so I

577
00:26:38,230 --> 00:26:41,860
want to write the writer is invalidating

578
00:26:41,860 --> 00:26:43,780
the key in memcache do you know what

579
00:26:43,780 --> 00:26:47,380
that means is that the next front-end

580
00:26:47,380 --> 00:26:49,000
that tries to read that key from

581
00:26:49,000 --> 00:26:52,180
memcache D is gonna get nil back because

582
00:26:52,180 --> 00:26:54,130
it's no longer cached and will fetch the

583
00:26:54,130 --> 00:26:56,500
updated value from the database and

584
00:26:56,500 --> 00:27:01,570
install it into memcache all right so

585
00:27:01,570 --> 00:27:04,210
this is an invalidation in particular

586
00:27:04,210 --> 00:27:06,490
it's not you could imagine a scheme that

587
00:27:06,490 --> 00:27:08,620
would send the new data to memcache T at

588
00:27:08,620 --> 00:27:10,000
this point but it doesn't actually do

589
00:27:10,000 --> 00:27:13,180
that instead of gliese it and actually

590
00:27:13,180 --> 00:27:17,830
in the context of facebook scheme the

591
00:27:17,830 --> 00:27:21,640
real reason why this delete is needed is

592
00:27:21,640 --> 00:27:24,330
so that

593
00:27:24,330 --> 00:27:26,940
we'll see their own rights because in

594
00:27:26,940 --> 00:27:29,940
fact in their scheme the mem cat the my

595
00:27:29,940 --> 00:27:32,490
sequel server the database servers also

596
00:27:32,490 --> 00:27:35,460
send deletes one of you and the front

597
00:27:35,460 --> 00:27:37,500
end writes something in the database the

598
00:27:37,500 --> 00:27:39,809
database with the mix squeal mechanism

599
00:27:39,809 --> 00:27:42,029
the paper mentions well send the

600
00:27:42,029 --> 00:27:44,730
relevant deletes to the memcache servers

601
00:27:44,730 --> 00:27:47,159
that that might hold this key so the

602
00:27:47,159 --> 00:27:49,350
data the database servers will actually

603
00:27:49,350 --> 00:27:51,510
invalidate stuff in memcache by-and-bye

604
00:27:51,510 --> 00:27:54,539
may take them a while um but because

605
00:27:54,539 --> 00:27:57,179
that might take a while the front ends

606
00:27:57,179 --> 00:27:58,980
also delete the key said that a front

607
00:27:58,980 --> 00:28:04,649
end won't see a stale value for data

608
00:28:04,649 --> 00:28:14,659
that it just updated okay

609
00:28:16,590 --> 00:28:21,390
all sort of the background of this is

610
00:28:21,390 --> 00:28:23,520
pretty much how everybody uses memcache

611
00:28:23,520 --> 00:28:24,990
G there's nothing yet really very

612
00:28:24,990 --> 00:28:28,980
special here now eventually you know the

613
00:28:28,980 --> 00:28:31,350
paper is all about on the surface all

614
00:28:31,350 --> 00:28:34,020
about solving consistency problems and

615
00:28:34,020 --> 00:28:36,929
indeed those are important but the

616
00:28:36,929 --> 00:28:39,059
reason why they got where they ran into

617
00:28:39,059 --> 00:28:41,700
those consistency problems is in large

618
00:28:41,700 --> 00:28:46,380
part because they you know modify the

619
00:28:46,380 --> 00:28:47,970
design or set up a design that had

620
00:28:47,970 --> 00:28:49,289
extremely high performance because they

621
00:28:49,289 --> 00:28:51,330
had extremely high load and say they

622
00:28:51,330 --> 00:28:54,029
were desperate to get performance and

623
00:28:54,029 --> 00:28:56,909
kind of struggled along behind the

624
00:28:56,909 --> 00:28:59,840
performance improvements in order to

625
00:28:59,840 --> 00:29:02,100
retain a reasonable level of consistency

626
00:29:02,100 --> 00:29:04,110
and because the performance kind of came

627
00:29:04,110 --> 00:29:05,610
first for them I'm actually going to

628
00:29:05,610 --> 00:29:07,590
talk about

629
00:29:07,590 --> 00:29:10,529
their performance architecture before

630
00:29:10,529 --> 00:29:14,210
talking about how they fix the

631
00:29:14,210 --> 00:29:18,600
consistency okay sorry there's been a

632
00:29:18,600 --> 00:29:21,330
bunch of questions here that I haven't

633
00:29:21,330 --> 00:29:28,559
seen let me take a peek okay so one

634
00:29:28,559 --> 00:29:30,029
question this means that the replicated

635
00:29:30,029 --> 00:29:32,490
updates from the primary my sequel

636
00:29:32,490 --> 00:29:34,500
database to the secondary must also

637
00:29:34,500 --> 00:29:39,210
issue deletes - yeah so this is I think

638
00:29:39,210 --> 00:29:40,860
a reference to the previous or

639
00:29:40,860 --> 00:29:43,230
architecture slide the observation is

640
00:29:43,230 --> 00:29:47,100
that yes indeed when a front-end sends a

641
00:29:47,100 --> 00:29:49,500
write to the database server today every

642
00:29:49,500 --> 00:29:53,039
server updates its data on disk and it

643
00:29:53,039 --> 00:29:56,070
will send an invalidate a delete to

644
00:29:56,070 --> 00:29:58,260
whatever memcache server there is in the

645
00:29:58,260 --> 00:30:00,690
local region the local data center that

646
00:30:00,690 --> 00:30:02,460
might have had the key that was just

647
00:30:02,460 --> 00:30:05,070
updated the database server also sends a

648
00:30:05,070 --> 00:30:07,950
sort of representation of the update to

649
00:30:07,950 --> 00:30:09,899
the corresponding database serve in the

650
00:30:09,899 --> 00:30:11,940
other region which process it applies

651
00:30:11,940 --> 00:30:15,090
the right to its disk data on disk it

652
00:30:15,090 --> 00:30:18,779
also using them excuse sort of log

653
00:30:18,779 --> 00:30:22,169
reading apparatus figures out which

654
00:30:22,169 --> 00:30:25,169
memcache server might hold the key that

655
00:30:25,169 --> 00:30:27,299
was just updated and sends it delete

656
00:30:27,299 --> 00:30:32,340
also to that memcache server so that the

657
00:30:32,340 --> 00:30:34,740
if it's the key is cache is invalidated

658
00:30:34,740 --> 00:30:40,049
in in both data centers okay so another

659
00:30:40,049 --> 00:30:41,399
question what would happen if we delete

660
00:30:41,399 --> 00:30:45,210
first in the right and then send to the

661
00:30:45,210 --> 00:30:47,630
database

662
00:30:48,260 --> 00:30:50,960
so that's or with reference to this

663
00:30:50,960 --> 00:30:53,190
thing here would what if we did to the

664
00:30:53,190 --> 00:30:58,830
feet first you know if you do delete

665
00:30:58,830 --> 00:31:01,289
first then you're increasing the chances

666
00:31:01,289 --> 00:31:03,659
that some other clients so supposing you

667
00:31:03,659 --> 00:31:11,100
delete and then send to database right

668
00:31:11,100 --> 00:31:13,020
in here if another client reads that

669
00:31:13,020 --> 00:31:14,880
same key they're gonna miss at this

670
00:31:14,880 --> 00:31:18,480
point they're gonna fetch the old data

671
00:31:18,480 --> 00:31:20,460
from the database and they're gonna then

672
00:31:20,460 --> 00:31:21,240
insert it

673
00:31:21,240 --> 00:31:22,980
cash and then you're going to update it

674
00:31:22,980 --> 00:31:25,490
leaving memcache for a while at least

675
00:31:25,490 --> 00:31:28,740
with stale data and then if this the

676
00:31:28,740 --> 00:31:30,120
writing client reads it again it may see

677
00:31:30,120 --> 00:31:31,440
the stale data even though it just

678
00:31:31,440 --> 00:31:34,820
updated it doing the delete second um

679
00:31:34,820 --> 00:31:37,290
you know these over the possibility that

680
00:31:37,290 --> 00:31:40,410
somebody will read during this period of

681
00:31:40,410 --> 00:31:43,350
time and see steal data but they're not

682
00:31:43,350 --> 00:31:44,550
worried about stale data in general

683
00:31:44,550 --> 00:31:47,130
they're really most worried in this

684
00:31:47,130 --> 00:31:50,400
context about clients reading their own

685
00:31:50,400 --> 00:31:52,950
rights so on balance even though there's

686
00:31:52,950 --> 00:31:56,220
a consistency problem by the way I'm

687
00:31:56,220 --> 00:32:00,240
doing the delete second ensures that

688
00:32:00,240 --> 00:32:02,850
clients will be their own rights in

689
00:32:02,850 --> 00:32:06,450
either case eventually the database

690
00:32:06,450 --> 00:32:09,720
server as I'm just mentioned will send a

691
00:32:09,720 --> 00:32:13,190
delete for the written keys

692
00:32:19,510 --> 00:32:21,560
another question I'm confused on how

693
00:32:21,560 --> 00:32:24,670
writing the new value shows stale data

694
00:32:24,670 --> 00:32:40,940
but deleting doesn't let me see I'm not

695
00:32:40,940 --> 00:32:42,500
really sure what the question is asking

696
00:32:42,500 --> 00:32:47,770
the if it's with reference to this code

697
00:32:47,770 --> 00:32:53,090
once the writes done okay maybe the

698
00:32:53,090 --> 00:32:54,560
question is it's really we didn't do

699
00:32:54,560 --> 00:32:59,060
delete at all so that when a client a

700
00:32:59,060 --> 00:33:02,180
front web for an end did or wanted to

701
00:33:02,180 --> 00:33:03,350
update him David would just tell the

702
00:33:03,350 --> 00:33:07,550
database but not explicitly delete the

703
00:33:07,550 --> 00:33:10,420
data from memcache the problem with this

704
00:33:10,420 --> 00:33:17,210
is that if the client sent this write to

705
00:33:17,210 --> 00:33:19,430
the database and then immediately read

706
00:33:19,430 --> 00:33:22,040
the same data that read would come out

707
00:33:22,040 --> 00:33:24,830
of the memcache and because memcache

708
00:33:24,830 --> 00:33:26,240
still has the old data

709
00:33:26,240 --> 00:33:27,590
you know memcache hasn't seen this right

710
00:33:27,590 --> 00:33:30,800
yet a client that updated some data and

711
00:33:30,800 --> 00:33:32,900
then read it you know updates it in the

712
00:33:32,900 --> 00:33:34,370
database but it reads the data if the

713
00:33:34,370 --> 00:33:36,320
stale data from memcache and then a

714
00:33:36,320 --> 00:33:38,450
client might update some data but still

715
00:33:38,450 --> 00:33:41,570
see the old data and if you delete it

716
00:33:41,570 --> 00:33:44,330
from memcache then if a client if you do

717
00:33:44,330 --> 00:33:46,640
do this delete than a client that writes

718
00:33:46,640 --> 00:33:49,760
some data and deletes it from memcache

719
00:33:49,760 --> 00:33:51,800
and then reads it again it'll miss in

720
00:33:51,800 --> 00:33:54,110
memcache because of the delete and they

721
00:33:54,110 --> 00:33:55,100
don't have to go to the database and

722
00:33:55,100 --> 00:33:56,780
read the data and the database will give

723
00:33:56,780 --> 00:34:03,560
it fresh data okay so the question is

724
00:34:03,560 --> 00:34:06,740
how come why do we delete here

725
00:34:06,740 --> 00:34:10,730
gosh why don't we just instead of this

726
00:34:10,730 --> 00:34:13,880
delete have the client just directly

727
00:34:13,880 --> 00:34:18,260
since it knows the new data just send a

728
00:34:18,260 --> 00:34:24,080
set RPC - memcache T and this is a this

729
00:34:24,080 --> 00:34:24,949
is a good question

730
00:34:24,949 --> 00:34:26,960
and so here we're doing I have an

731
00:34:26,960 --> 00:34:29,090
invalidate scheme this would often be

732
00:34:29,090 --> 00:34:34,760
called an update scheme and let me try

733
00:34:34,760 --> 00:34:37,370
to cook up an example that shows that

734
00:34:37,370 --> 00:34:39,110
while this could probably be made to

735
00:34:39,110 --> 00:34:44,980
work this update scheme it doesn't work

736
00:34:45,130 --> 00:34:47,330
it doesn't work out of the box and you

737
00:34:47,330 --> 00:34:48,800
wouldn't you need to do some careful

738
00:34:48,800 --> 00:34:50,570
design in order to make it work so this

739
00:34:50,570 --> 00:34:51,889
wasn't client wants posing up now we

740
00:34:51,889 --> 00:34:54,819
have two clients

741
00:34:57,220 --> 00:34:59,130
reading and writing the same key

742
00:34:59,130 --> 00:35:05,760
interleaved so let's say client one

743
00:35:05,760 --> 00:35:11,710
tells the database you know sends X plus

744
00:35:11,710 --> 00:35:15,130
plus to the database right just

745
00:35:15,130 --> 00:35:19,630
incrementing X and then of course or let

746
00:35:19,630 --> 00:35:22,990
me say it's going to increment X from

747
00:35:22,990 --> 00:35:25,980
zero to one so set X to one and then

748
00:35:25,980 --> 00:35:30,490
after that client one is going to call

749
00:35:30,490 --> 00:35:36,400
set of our key which is X and the value

750
00:35:36,400 --> 00:35:40,050
one and write that at the memcache D

751
00:35:40,050 --> 00:35:43,240
supposing meanwhile client two also

752
00:35:43,240 --> 00:35:46,150
wants to increment X so it's going to

753
00:35:46,150 --> 00:35:48,010
read this latest value in the database

754
00:35:48,010 --> 00:35:50,700
and almost certainly these are in fact

755
00:35:50,700 --> 00:35:54,070
transactions so what if we were doing

756
00:35:54,070 --> 00:35:55,450
increment what client won't won't be

757
00:35:55,450 --> 00:35:57,640
sending would be some sort of increment

758
00:35:57,640 --> 00:35:58,810
transaction on the database for

759
00:35:58,810 --> 00:36:00,400
correctness because the database does

760
00:36:00,400 --> 00:36:02,710
support transactions so we're going to

761
00:36:02,710 --> 00:36:04,270
imagine the client to increments the

762
00:36:04,270 --> 00:36:07,839
value of x to to sends that increment to

763
00:36:07,839 --> 00:36:09,550
the database and client two also is

764
00:36:09,550 --> 00:36:12,070
going to do this set so it's going to

765
00:36:12,070 --> 00:36:15,970
set X to be two but now what we're left

766
00:36:15,970 --> 00:36:19,690
with is the value of one in memcache D

767
00:36:19,690 --> 00:36:21,700
even though the correct values and the

768
00:36:21,700 --> 00:36:25,540
databases to which is to say if we do

769
00:36:25,540 --> 00:36:28,060
this update was set even though it does

770
00:36:28,060 --> 00:36:29,560
save us some time right cuz now we're

771
00:36:29,560 --> 00:36:31,720
saving somebody a miss in the future

772
00:36:31,720 --> 00:36:33,099
because we directly said instead of

773
00:36:33,099 --> 00:36:36,130
delete we also run the risk if the if

774
00:36:36,130 --> 00:36:39,040
it's popular data of leaving stale data

775
00:36:39,040 --> 00:36:41,770
in the database it's not that you

776
00:36:41,770 --> 00:36:45,849
couldn't get this to work somehow but it

777
00:36:45,849 --> 00:36:49,599
does require some careful thought to fix

778
00:36:49,599 --> 00:36:55,599
this problem all right so that was why

779
00:36:55,599 --> 00:36:58,450
they use invalidate and instead of

780
00:36:58,450 --> 00:37:03,109
update okay so I was going to

781
00:37:03,109 --> 00:37:07,460
about performance they this sort of

782
00:37:07,460 --> 00:37:09,789
route of how they get performance is

783
00:37:09,789 --> 00:37:12,859
through parallel parallelization

784
00:37:12,859 --> 00:37:16,279
parallel execution and for a storage

785
00:37:16,279 --> 00:37:18,650
system just at a high level there's

786
00:37:18,650 --> 00:37:21,499
really two ways that you can get a good

787
00:37:21,499 --> 00:37:22,249
performance

788
00:37:22,249 --> 00:37:27,400
one is by partition which is sharding

789
00:37:27,400 --> 00:37:30,049
that is you take your data and you split

790
00:37:30,049 --> 00:37:32,450
it up over you know into ten pieces over

791
00:37:32,450 --> 00:37:34,069
ten servers and those ten servers can

792
00:37:34,069 --> 00:37:37,190
run independently hopefully the other

793
00:37:37,190 --> 00:37:39,619
way you can use extra hardware to get

794
00:37:39,619 --> 00:37:42,579
higher performance despite replication

795
00:37:42,579 --> 00:37:48,140
at is have more than one copy of the

796
00:37:48,140 --> 00:37:51,109
data and you kind of for a given amount

797
00:37:51,109 --> 00:37:53,259
of hardware you can kind of choose

798
00:37:53,259 --> 00:37:55,670
whether to partition your data or

799
00:37:55,670 --> 00:37:56,930
replicate it in order to use that

800
00:37:56,930 --> 00:38:04,519
hardware and there's you know from

801
00:38:04,519 --> 00:38:05,509
memcache see what we're talking about

802
00:38:05,509 --> 00:38:09,529
here is is splitting the data over the

803
00:38:09,529 --> 00:38:11,599
available memcache servers by hashing

804
00:38:11,599 --> 00:38:14,509
the key so that every key sort of lives

805
00:38:14,509 --> 00:38:16,279
on one memcache server and from memcache

806
00:38:16,279 --> 00:38:18,789
what we would be talking about here is

807
00:38:18,789 --> 00:38:21,769
having each front-end just talk to a

808
00:38:21,769 --> 00:38:23,839
single memcache server and send all its

809
00:38:23,839 --> 00:38:25,819
requests there so that each memcache

810
00:38:25,819 --> 00:38:29,029
server serves only a subset of the front

811
00:38:29,029 --> 00:38:31,690
ends and sort of serves all their needs

812
00:38:31,690 --> 00:38:35,599
and Facebook actually uses a combination

813
00:38:35,599 --> 00:38:39,019
of both partition and replication for

814
00:38:39,019 --> 00:38:41,390
partition the things that are in its

815
00:38:41,390 --> 00:38:44,349
favor one is that it's memory efficient

816
00:38:44,349 --> 00:38:48,220
because you only store a single copy of

817
00:38:48,220 --> 00:38:50,720
each item Abita where's in replication

818
00:38:50,720 --> 00:38:52,519
you're gonna store every piece of data

819
00:38:52,519 --> 00:39:00,440
maybe on every server on the sort of

820
00:39:00,440 --> 00:39:03,290
partition is that it's as long as your

821
00:39:03,290 --> 00:39:05,329
keys are sort of equally roughly equally

822
00:39:05,329 --> 00:39:07,970
popular works pretty well but if there's

823
00:39:07,970 --> 00:39:10,040
some hot a few hot keys partition

824
00:39:10,040 --> 00:39:12,140
doesn't really help you much once you

825
00:39:12,140 --> 00:39:13,609
get those partition enough that those

826
00:39:13,609 --> 00:39:17,690
hot keys are on different servers you

827
00:39:17,690 --> 00:39:19,160
know once the if there's a single hot

828
00:39:19,160 --> 00:39:20,690
key for example no amount of

829
00:39:20,690 --> 00:39:22,910
partitioning helps you because no matter

830
00:39:22,910 --> 00:39:25,220
of how much you partition that hot key

831
00:39:25,220 --> 00:39:30,550
is still sitting on just one server

832
00:39:34,100 --> 00:39:38,640
the problem partition is that it doesn't

833
00:39:38,640 --> 00:39:40,890
mean that the front if front ends need

834
00:39:40,890 --> 00:39:42,930
to use lots of data lots of different

835
00:39:42,930 --> 00:39:44,970
keys it means in the end each front-end

836
00:39:44,970 --> 00:39:47,580
is probably going to talk to lots of

837
00:39:47,580 --> 00:39:51,060
partitions and at least if you use

838
00:39:51,060 --> 00:39:53,270
protocols like TCP that keep state

839
00:39:53,270 --> 00:39:58,530
there's significant overhead to as you

840
00:39:58,530 --> 00:40:00,090
add more and more sort of N squared

841
00:40:00,090 --> 00:40:09,480
communication for a replication it's

842
00:40:09,480 --> 00:40:12,830
fantastic if if your problem is that a

843
00:40:12,830 --> 00:40:17,160
few keys are popular because now you

844
00:40:17,160 --> 00:40:18,390
know you're making replicas of those

845
00:40:18,390 --> 00:40:20,369
those hotkeys and you can serve each

846
00:40:20,369 --> 00:40:24,210
replica the same key in parallel it's

847
00:40:24,210 --> 00:40:26,490
good because there's fewer this there's

848
00:40:26,490 --> 00:40:28,980
not n squared communication each

849
00:40:28,980 --> 00:40:30,270
front-end maybe only talks to one

850
00:40:30,270 --> 00:40:38,270
memcache server but the bad thing is

851
00:40:38,270 --> 00:40:40,220
it's there's a copy of data in every

852
00:40:40,220 --> 00:40:43,580
server you can cache far fewer distinct

853
00:40:43,580 --> 00:40:47,600
data items with replication then with

854
00:40:47,600 --> 00:40:53,270
partition so there's less total data can

855
00:40:53,270 --> 00:40:57,140
be stored so these are just generic for

856
00:40:57,140 --> 00:41:00,890
pros and cons of these two main ways of

857
00:41:00,890 --> 00:41:02,990
using extra hardware to get higher

858
00:41:02,990 --> 00:41:07,670
performance alright so I want to talk a

859
00:41:07,670 --> 00:41:11,150
bit about there when one sort of context

860
00:41:11,150 --> 00:41:12,710
in which they use partition and

861
00:41:12,710 --> 00:41:15,620
replication is at the level of different

862
00:41:15,620 --> 00:41:21,820
regions so I just want to talk through

863
00:41:21,820 --> 00:41:26,110
why it is that they decided to have

864
00:41:26,110 --> 00:41:28,910
separate regions and kind of separate

865
00:41:28,910 --> 00:41:30,560
complete data center with all the data

866
00:41:30,560 --> 00:41:33,950
in each of the regions so I before I do

867
00:41:33,950 --> 00:41:35,990
that there's a question why can't we

868
00:41:35,990 --> 00:41:38,330
cache the same amount of data with

869
00:41:38,330 --> 00:41:42,170
replication ok so supposing you have 10

870
00:41:42,170 --> 00:41:46,070
machines each with a gigabyte of RAM and

871
00:41:46,070 --> 00:41:48,380
you can use these 10 machines each with

872
00:41:48,380 --> 00:41:50,240
a gigabyte of RAM for either replication

873
00:41:50,240 --> 00:41:53,960
or in a partitioning scheme if you use a

874
00:41:53,960 --> 00:41:56,290
partitioning scheme where each server

875
00:41:56,290 --> 00:41:58,310
stores different data from the other

876
00:41:58,310 --> 00:42:01,160
servers that you can store a total of 10

877
00:42:01,160 --> 00:42:04,250
gigabytes of distinct data objects on

878
00:42:04,250 --> 00:42:07,070
your 10 servers each with a gigabyte of

879
00:42:07,070 --> 00:42:09,680
RAM so with partition you know each byte

880
00:42:09,680 --> 00:42:11,270
of ram is used for different data so you

881
00:42:11,270 --> 00:42:12,470
can look at the total amount of RAM you

882
00:42:12,470 --> 00:42:15,470
have that's how much distinct data you

883
00:42:15,470 --> 00:42:16,940
know different data items you can store

884
00:42:16,940 --> 00:42:20,510
with replication you know assuming your

885
00:42:20,510 --> 00:42:22,400
users are more or less looking at the

886
00:42:22,400 --> 00:42:29,690
same stuff each each replicas each cache

887
00:42:29,690 --> 00:42:32,240
replicas will end up storing roughly the

888
00:42:32,240 --> 00:42:36,440
same stuff as all the other caches so

889
00:42:36,440 --> 00:42:38,060
your 10 you have 10 gigabytes of RAM

890
00:42:38,060 --> 00:42:41,270
still they and your 10 machines but each

891
00:42:41,270 --> 00:42:42,920
of those machines stores roughly the

892
00:42:42,920 --> 00:42:44,420
same data so would you end up with this

893
00:42:44,420 --> 00:42:48,460
10 copies of the same gigabyte of items

894
00:42:48,460 --> 00:42:51,110
so in a can this particular example if

895
00:42:51,110 --> 00:42:52,090
you use replication you

896
00:42:52,090 --> 00:42:54,100
snoring attempt as many distinct data

897
00:42:54,100 --> 00:42:57,010
items and you know that may actually be

898
00:42:57,010 --> 00:43:01,450
a good idea depending on you know sort

899
00:43:01,450 --> 00:43:04,480
of way your data is like but it does

900
00:43:04,480 --> 00:43:07,090
mean that replication gives you less

901
00:43:07,090 --> 00:43:09,490
total data that's cached and you know

902
00:43:09,490 --> 00:43:11,140
you can see there's points in the paper

903
00:43:11,140 --> 00:43:14,530
word that they mention this tension

904
00:43:14,530 --> 00:43:18,040
nominally they don't come down on one

905
00:43:18,040 --> 00:43:19,150
side of the other because they use both

906
00:43:19,150 --> 00:43:27,610
replication and charting okay okay so

907
00:43:27,610 --> 00:43:30,100
the highest level at which they're

908
00:43:30,100 --> 00:43:33,450
playing this game is between regions and

909
00:43:33,450 --> 00:43:36,730
so it at this high level each region has

910
00:43:36,730 --> 00:43:38,880
a complete replica of all the data right

911
00:43:38,880 --> 00:43:41,200
they have a each region as a complete

912
00:43:41,200 --> 00:43:42,850
set of database servers each database

913
00:43:42,850 --> 00:43:45,340
database corresponding database servers

914
00:43:45,340 --> 00:43:47,890
for the same data and assuming users are

915
00:43:47,890 --> 00:43:49,150
looking at more or less the same stuff

916
00:43:49,150 --> 00:43:52,630
that means the memcache servers in the

917
00:43:52,630 --> 00:43:55,030
different regions are also storing more

918
00:43:55,030 --> 00:43:57,580
or less basically replicating where we

919
00:43:57,580 --> 00:43:59,350
have yours replicating in both the

920
00:43:59,350 --> 00:44:00,760
database servers and the memcache

921
00:44:00,760 --> 00:44:04,330
servers and the point again one point is

922
00:44:04,330 --> 00:44:09,400
to you want a complete copy of the site

923
00:44:09,400 --> 00:44:11,590
that's close to West Coast users in the

924
00:44:11,590 --> 00:44:13,960
internet load early in the internet and

925
00:44:13,960 --> 00:44:16,030
another copy of the complete website

926
00:44:16,030 --> 00:44:18,460
this close to users on the East Coast

927
00:44:18,460 --> 00:44:22,240
close on the internet again and the

928
00:44:22,240 --> 00:44:23,860
Internet's pretty fast but coast to

929
00:44:23,860 --> 00:44:27,460
coast is you know 50 milliseconds or

930
00:44:27,460 --> 00:44:30,070
something which if you do if users have

931
00:44:30,070 --> 00:44:31,770
to wait too many 50 millisecond

932
00:44:31,770 --> 00:44:33,820
intervals they'll start to notice that

933
00:44:33,820 --> 00:44:36,880
amount of time another reason is that

934
00:44:36,880 --> 00:44:42,800
the you wanna a reason to

935
00:44:42,800 --> 00:44:44,930
applicate the data between the two

936
00:44:44,930 --> 00:44:48,200
regions is that these front ends to even

937
00:44:48,200 --> 00:44:50,740
create a single web page for user

938
00:44:50,740 --> 00:44:53,840
requests often dozens or hundreds of

939
00:44:53,840 --> 00:44:55,880
distinct data items from the cache or

940
00:44:55,880 --> 00:44:58,940
the databases and so the speed the

941
00:44:58,940 --> 00:45:00,920
latency the delay at which a front-end

942
00:45:00,920 --> 00:45:03,770
can fetch these hundreds of items from

943
00:45:03,770 --> 00:45:05,420
that from the look from the memcache key

944
00:45:05,420 --> 00:45:07,880
is quite important and so it's extremely

945
00:45:07,880 --> 00:45:10,610
important to have the front and only

946
00:45:10,610 --> 00:45:15,080
talk to only read local memcache servers

947
00:45:15,080 --> 00:45:17,960
and local databases so that you can do

948
00:45:17,960 --> 00:45:19,160
the hundreds of queries it needs to do

949
00:45:19,160 --> 00:45:21,350
for a web page very rapidly so if we

950
00:45:21,350 --> 00:45:23,090
have partitioned the data between the

951
00:45:23,090 --> 00:45:27,110
two regions then a front-end you know if

952
00:45:27,110 --> 00:45:28,880
I'm looking at my friends and some of my

953
00:45:28,880 --> 00:45:29,990
friends are on the East Coast and some

954
00:45:29,990 --> 00:45:31,880
on the west coast that means if we

955
00:45:31,880 --> 00:45:33,860
partitioned that would might require the

956
00:45:33,860 --> 00:45:37,120
front ends to actually make many

957
00:45:37,120 --> 00:45:39,820
requests you know 50 milliseconds each

958
00:45:39,820 --> 00:45:45,280
to the other data center and users would

959
00:45:45,280 --> 00:45:49,280
users would see this kind of latency and

960
00:45:49,280 --> 00:45:52,130
be very upset so so the reason to

961
00:45:52,130 --> 00:45:53,720
another reason to replicate is to keep

962
00:45:53,720 --> 00:45:56,300
the front ends always close to the data

963
00:45:56,300 --> 00:46:00,140
to all the data they need of course this

964
00:46:00,140 --> 00:46:01,460
makes writes more expensive because now

965
00:46:01,460 --> 00:46:03,500
if a front-end and the secondary region

966
00:46:03,500 --> 00:46:05,360
needs to write in estes send the data

967
00:46:05,360 --> 00:46:07,940
all the way across the internet the

968
00:46:07,940 --> 00:46:10,370
reads are far far more frequent than

969
00:46:10,370 --> 00:46:13,850
right so it's a good trade-off although

970
00:46:13,850 --> 00:46:15,350
the paper doesn't mention it it's

971
00:46:15,350 --> 00:46:18,200
possible that another reason for

972
00:46:18,200 --> 00:46:20,180
complete replication between the two

973
00:46:20,180 --> 00:46:23,720
sites is so that if the primary site

974
00:46:23,720 --> 00:46:26,030
goes down perhaps they could switch the

975
00:46:26,030 --> 00:46:27,560
whole operation to the secondary site

976
00:46:27,560 --> 00:46:29,360
but I don't know if they had that in

977
00:46:29,360 --> 00:46:31,570
mind

978
00:46:34,890 --> 00:46:38,650
okay so this is the story between

979
00:46:38,650 --> 00:46:40,359
Regents is basically a story of

980
00:46:40,359 --> 00:46:45,359
replication between the two data centers

981
00:46:48,150 --> 00:46:51,239
all right now within a data center

982
00:46:51,239 --> 00:47:00,269
within a region so in each region

983
00:47:00,269 --> 00:47:06,239
there's a single set of database servers

984
00:47:07,620 --> 00:47:11,900
so at the database level the data is

985
00:47:11,900 --> 00:47:14,910
charted and not replicated inside each

986
00:47:14,910 --> 00:47:19,440
region however at the memcache level

987
00:47:19,440 --> 00:47:21,060
they actually use replication as well as

988
00:47:21,060 --> 00:47:22,590
charting so they had this notion of

989
00:47:22,590 --> 00:47:26,190
clusters so a given regions actually

990
00:47:26,190 --> 00:47:29,820
supports multiple clusters of front-ends

991
00:47:29,820 --> 00:47:31,680
and database servers so here I'm going

992
00:47:31,680 --> 00:47:34,020
to have two clusters in this region this

993
00:47:34,020 --> 00:47:35,670
cluster has a you know a bunch of front

994
00:47:35,670 --> 00:47:40,940
ends and a bunch of memcache servers and

995
00:47:40,940 --> 00:47:45,690
these are completely independent almost

996
00:47:45,690 --> 00:47:46,950
completely independent so that a

997
00:47:46,950 --> 00:47:49,440
front-end and cluster one sends all its

998
00:47:49,440 --> 00:47:51,990
reads to the local memcache servers and

999
00:47:51,990 --> 00:47:54,300
misses it needs to go to the one instead

1000
00:47:54,300 --> 00:47:57,480
of database servers and similarly each

1001
00:47:57,480 --> 00:48:01,340
front-end in this cluster

1002
00:48:02,680 --> 00:48:06,080
talks only to memcache servers in the

1003
00:48:06,080 --> 00:48:11,480
same cluster so why do they have this

1004
00:48:11,480 --> 00:48:15,380
multiple clusters why not just have you

1005
00:48:15,380 --> 00:48:16,580
know essentially a single cluster a

1006
00:48:16,580 --> 00:48:18,530
single set of front end servers and a

1007
00:48:18,530 --> 00:48:20,690
single set of memcache server is shared

1008
00:48:20,690 --> 00:48:24,140
by all those front ends one is that if

1009
00:48:24,140 --> 00:48:26,180
you did that and and that would mean you

1010
00:48:26,180 --> 00:48:27,830
know if you need to scale up capacity

1011
00:48:27,830 --> 00:48:29,360
you sort of be adding more and more

1012
00:48:29,360 --> 00:48:31,820
memcache servers in front ends to the

1013
00:48:31,820 --> 00:48:36,230
same cluster you don't get any win

1014
00:48:36,230 --> 00:48:38,990
therefore in performance for popular

1015
00:48:38,990 --> 00:48:43,100
Keys you know so there the data sort of

1016
00:48:43,100 --> 00:48:44,570
this memcache service is sort of a mix

1017
00:48:44,570 --> 00:48:46,400
you know most of it is maybe only used

1018
00:48:46,400 --> 00:48:48,140
by a small number of users but there's

1019
00:48:48,140 --> 00:48:49,790
some stuff there that lots and lots of

1020
00:48:49,790 --> 00:48:52,070
users need to look at and by using

1021
00:48:52,070 --> 00:48:55,370
replication as well as sharding they get

1022
00:48:55,370 --> 00:48:57,920
you know multiple copies of the very

1023
00:48:57,920 --> 00:49:00,800
popular keys and therefore they get sort

1024
00:49:00,800 --> 00:49:02,770
of parallel serving of those keys

1025
00:49:02,770 --> 00:49:07,880
between the different clusters another

1026
00:49:07,880 --> 00:49:11,360
reason to not want to increase the size

1027
00:49:11,360 --> 00:49:13,340
of the cluster individual cluster too

1028
00:49:13,340 --> 00:49:16,430
much is that all the data within a

1029
00:49:16,430 --> 00:49:19,550
cluster is spread over partitioned over

1030
00:49:19,550 --> 00:49:21,800
all the memcache servers and any one

1031
00:49:21,800 --> 00:49:23,780
front end is typically actually going to

1032
00:49:23,780 --> 00:49:26,420
need data from probably every single

1033
00:49:26,420 --> 00:49:30,650
memcache server eventually and so this

1034
00:49:30,650 --> 00:49:31,640
means you have a sort of n-squared

1035
00:49:31,640 --> 00:49:33,380
communication pattern between the front

1036
00:49:33,380 --> 00:49:38,060
ends and the memcache servers and to the

1037
00:49:38,060 --> 00:49:39,590
extent that they're using TCP for the

1038
00:49:39,590 --> 00:49:41,360
communication that involves a lot of

1039
00:49:41,360 --> 00:49:44,180
overhead a lot of sort of connection

1040
00:49:44,180 --> 00:49:46,340
state for all the different TCP so they

1041
00:49:46,340 --> 00:49:50,120
wanted to limit so you know this is N

1042
00:49:50,120 --> 00:49:55,400
squared CCP's they want to limit the

1043
00:49:55,400 --> 00:49:57,440
growth of this and the way to do that is

1044
00:49:57,440 --> 00:49:58,970
to make sure that no one cluster gets to

1045
00:49:58,970 --> 00:50:01,640
be too big so this N squared doesn't get

1046
00:50:01,640 --> 00:50:03,970
too large

1047
00:50:09,010 --> 00:50:13,060
and well related to that is this in

1048
00:50:13,060 --> 00:50:14,740
caste congestion business they're

1049
00:50:14,740 --> 00:50:17,290
talking about the if a frontman needs

1050
00:50:17,290 --> 00:50:20,530
data from lots of memcache servers it's

1051
00:50:20,530 --> 00:50:21,670
actually it's gonna send out the

1052
00:50:21,670 --> 00:50:23,290
requests more or less all at the same

1053
00:50:23,290 --> 00:50:25,270
time and that means this front-end is

1054
00:50:25,270 --> 00:50:26,860
gonna get the responses from all the

1055
00:50:26,860 --> 00:50:28,480
memcache servers to query it more or

1056
00:50:28,480 --> 00:50:30,520
less the same side time and that may

1057
00:50:30,520 --> 00:50:32,170
mean dozens or hundreds of packets

1058
00:50:32,170 --> 00:50:34,630
arriving here all at the same time which

1059
00:50:34,630 --> 00:50:36,700
if you're not careful we'll cause packet

1060
00:50:36,700 --> 00:50:41,890
losses that's in caste congestion and in

1061
00:50:41,890 --> 00:50:43,510
order to limit how bad that was that you

1062
00:50:43,510 --> 00:50:44,740
had a bunch of techniques they talked

1063
00:50:44,740 --> 00:50:47,110
about but one of them was not making the

1064
00:50:47,110 --> 00:50:49,150
clusters too large so that the number of

1065
00:50:49,150 --> 00:50:51,520
memcache has given front-end tend to

1066
00:50:51,520 --> 00:50:53,650
talk to and they might be contributing

1067
00:50:53,650 --> 00:50:55,990
to the same caste never got to be too

1068
00:50:55,990 --> 00:50:59,650
large and a final reason the paper

1069
00:50:59,650 --> 00:51:02,200
mentions is that it's or behind this is

1070
00:51:02,200 --> 00:51:04,990
is a big network in the data center and

1071
00:51:04,990 --> 00:51:08,620
it's hard to build networks that are

1072
00:51:08,620 --> 00:51:11,860
both fast like many bits per second and

1073
00:51:11,860 --> 00:51:13,870
can talk to lots and lots of different

1074
00:51:13,870 --> 00:51:16,480
computers and by splitting the data

1075
00:51:16,480 --> 00:51:19,210
center up into these clusters and having

1076
00:51:19,210 --> 00:51:20,800
most of the communication go on just

1077
00:51:20,800 --> 00:51:22,720
within each cluster that means they need

1078
00:51:22,720 --> 00:51:25,210
a smaller they need you know a modest

1079
00:51:25,210 --> 00:51:27,370
size fast Network for this cluster and a

1080
00:51:27,370 --> 00:51:29,050
modest size you know reasonably fast

1081
00:51:29,050 --> 00:51:30,400
network for this cluster but they don't

1082
00:51:30,400 --> 00:51:32,050
have to build a single network that can

1083
00:51:32,050 --> 00:51:34,290
sort of handle all of the traffic

1084
00:51:34,290 --> 00:51:37,090
between among all the computers of the

1085
00:51:37,090 --> 00:51:41,220
giant cluster so it limits how expensive

1086
00:51:41,220 --> 00:51:44,620
underlying network is on the other hand

1087
00:51:44,620 --> 00:51:46,150
of course they're replicating the data

1088
00:51:46,150 --> 00:51:50,020
and the two clusters and for items that

1089
00:51:50,020 --> 00:51:51,430
aren't very popular and aren't really

1090
00:51:51,430 --> 00:51:53,500
going to benefit from the performance

1091
00:51:53,500 --> 00:51:58,120
win of having multiple copies this it's

1092
00:51:58,120 --> 00:52:00,370
wasteful to sit on all this RAM and you

1093
00:52:00,370 --> 00:52:01,450
know we're talking about hundreds or

1094
00:52:01,450 --> 00:52:03,700
thousands of servers so the amount of

1095
00:52:03,700 --> 00:52:05,110
money they spent on RAM for the memcache

1096
00:52:05,110 --> 00:52:10,540
services is no joke so in addition to

1097
00:52:10,540 --> 00:52:13,990
the pool of memcache servers inside each

1098
00:52:13,990 --> 00:52:17,220
cluster there's also this regional pool

1099
00:52:17,220 --> 00:52:20,680
of memcache servers that's

1100
00:52:20,680 --> 00:52:23,109
shared by all the clusters in a region

1101
00:52:23,109 --> 00:52:28,329
and into this regional pool they then

1102
00:52:28,329 --> 00:52:30,609
modify the software on the front end so

1103
00:52:30,609 --> 00:52:32,079
that the software on the front end knows

1104
00:52:32,079 --> 00:52:32,890
aha

1105
00:52:32,890 --> 00:52:35,980
this key the data for this skis actually

1106
00:52:35,980 --> 00:52:38,290
not use that often instead of storing it

1107
00:52:38,290 --> 00:52:41,410
on a memcache server my own cluster I'm

1108
00:52:41,410 --> 00:52:43,300
going to store this not very popular key

1109
00:52:43,300 --> 00:52:47,740
in the appropriate memcache server of

1110
00:52:47,740 --> 00:52:53,339
the regional pool so this is

1111
00:52:55,480 --> 00:53:00,369
the regional pool and this is just sort

1112
00:53:00,369 --> 00:53:02,339
of an admission that some data is not

1113
00:53:02,339 --> 00:53:04,420
popular enough to want to have lots of

1114
00:53:04,420 --> 00:53:06,670
replicas of it they can save money by

1115
00:53:06,670 --> 00:53:14,170
only cashing a single copy all right so

1116
00:53:14,170 --> 00:53:15,520
that's how they get that's this kind of

1117
00:53:15,520 --> 00:53:18,900
Carol replication versus partitioning

1118
00:53:18,900 --> 00:53:22,599
strategy they use inside each inside

1119
00:53:22,599 --> 00:53:26,170
each region a difficulty they had that

1120
00:53:26,170 --> 00:53:28,770
they discuss is that when they want to

1121
00:53:28,770 --> 00:53:32,079
create a new cluster in a data center

1122
00:53:32,079 --> 00:53:34,510
they actually have a sort of temporary

1123
00:53:34,510 --> 00:53:36,700
performance problem as they're getting

1124
00:53:36,700 --> 00:53:38,920
that cluster going so you know supposing

1125
00:53:38,920 --> 00:53:41,230
they decide to install you know couple

1126
00:53:41,230 --> 00:53:42,970
hundred machines to be a new cluster

1127
00:53:42,970 --> 00:53:44,799
with the front end new front ends new

1128
00:53:44,799 --> 00:53:47,829
memcache errors and then they fire it up

1129
00:53:47,829 --> 00:53:50,170
and you know maybe cause half the users

1130
00:53:50,170 --> 00:53:52,630
to start using the new cluster I'm gonna

1131
00:53:52,630 --> 00:53:55,030
have to use the old cluster well in the

1132
00:53:55,030 --> 00:53:56,710
beginning there's nothing in these

1133
00:53:56,710 --> 00:53:59,020
memcache servers and all the front end

1134
00:53:59,020 --> 00:54:00,220
servers are gonna miss on the memcache

1135
00:54:00,220 --> 00:54:04,390
servers and have to go to the databases

1136
00:54:04,390 --> 00:54:06,730
and at least at the beginning until

1137
00:54:06,730 --> 00:54:08,410
these memcache service gets populated

1138
00:54:08,410 --> 00:54:10,660
with all the sort of data that's used a

1139
00:54:10,660 --> 00:54:12,940
lot this is gonna increase the load on

1140
00:54:12,940 --> 00:54:15,010
the database servers absolutely enormous

1141
00:54:15,010 --> 00:54:18,099
leap because before we added the new

1142
00:54:18,099 --> 00:54:19,869
clusters maybe the database servers only

1143
00:54:19,869 --> 00:54:22,599
saw one percent of the reads because

1144
00:54:22,599 --> 00:54:24,280
maybe these memcache servers have a hit

1145
00:54:24,280 --> 00:54:27,069
rate of say 99 percent for reads the

1146
00:54:27,069 --> 00:54:28,420
only one percent of all that means go to

1147
00:54:28,420 --> 00:54:30,760
the database servers before we added the

1148
00:54:30,760 --> 00:54:33,520
new cluster if we add a new cluster with

1149
00:54:33,520 --> 00:54:35,530
nothing in the memcache servers and send

1150
00:54:35,530 --> 00:54:37,510
half the traffic to it it's gonna get a

1151
00:54:37,510 --> 00:54:39,839
hundred percent miss rate initially

1152
00:54:39,839 --> 00:54:44,500
right and so that'll mean you know we

1153
00:54:44,500 --> 00:54:46,780
gone from and so the overall miss write

1154
00:54:46,780 --> 00:54:48,490
will now be 50 percent so we've gone

1155
00:54:48,490 --> 00:54:51,609
from these database servers serving one

1156
00:54:51,609 --> 00:54:54,460
percent of the reads to them serving 50

1157
00:54:54,460 --> 00:54:56,410
percent of the reads so at least in this

1158
00:54:56,410 --> 00:54:58,530
imaginary example we've been quite

1159
00:54:58,530 --> 00:55:00,609
firing up this new cluster we may

1160
00:55:00,609 --> 00:55:02,559
increase the load on the databases by a

1161
00:55:02,559 --> 00:55:05,079
factor of 50 and chances are the

1162
00:55:05,079 --> 00:55:07,329
database servers were running you know

1163
00:55:07,329 --> 00:55:09,130
reasonably Coast's the capacity and

1164
00:55:09,130 --> 00:55:12,009
certainly not a factor of 50/50 under

1165
00:55:12,009 --> 00:55:14,980
capacity and so this would be the

1166
00:55:14,980 --> 00:55:17,710
absolute end of the world if they just

1167
00:55:17,710 --> 00:55:20,589
fired up a new cluster like that and so

1168
00:55:20,589 --> 00:55:26,650
instead they have this cold start idea

1169
00:55:26,650 --> 00:55:29,950
in which a new cluster is sort of marked

1170
00:55:29,950 --> 00:55:33,690
by some flag somewhere as being in this

1171
00:55:33,690 --> 00:55:38,190
cold start state and in that situation

1172
00:55:38,190 --> 00:55:40,599
when a front end and the new cluster

1173
00:55:40,599 --> 00:55:45,099
misses that actually first first it has

1174
00:55:45,099 --> 00:55:48,369
its own local memcache if that says no I

1175
00:55:48,369 --> 00:55:49,990
don't have the data then the front end

1176
00:55:49,990 --> 00:55:51,910
we'll ask the corresponding memcache in

1177
00:55:51,910 --> 00:55:54,579
another cluster in some warm cluster

1178
00:55:54,579 --> 00:55:56,410
that already has the data for the data

1179
00:55:56,410 --> 00:55:58,299
if it's popular data chances are it'll

1180
00:55:58,299 --> 00:56:01,569
be cached my friend and will get its

1181
00:56:01,569 --> 00:56:05,529
data and then it will install it in the

1182
00:56:05,529 --> 00:56:08,680
local memcache and it's only if both

1183
00:56:08,680 --> 00:56:10,839
local memcache and the warm memcache

1184
00:56:10,839 --> 00:56:13,180
don't have the data that this is front

1185
00:56:13,180 --> 00:56:15,220
end and the new cluster will read from

1186
00:56:15,220 --> 00:56:20,680
the database servers and so this is it

1187
00:56:20,680 --> 00:56:22,000
and so they run in this kind of cold

1188
00:56:22,000 --> 00:56:24,490
mode for a little while the paper I

1189
00:56:24,490 --> 00:56:25,990
think mentions a couple hours until the

1190
00:56:25,990 --> 00:56:28,180
memcache servers source and the new

1191
00:56:28,180 --> 00:56:30,400
clusters start to have all the popular

1192
00:56:30,400 --> 00:56:32,650
data and then they can turn off this

1193
00:56:32,650 --> 00:56:35,740
cold feature and just use the local

1194
00:56:35,740 --> 00:56:42,660
cluster memcache alone it's alright

1195
00:56:42,660 --> 00:56:47,410
so another another load problem that the

1196
00:56:47,410 --> 00:56:49,630
paper talks about if they ran into and

1197
00:56:49,630 --> 00:56:52,539
this is a load problem again deriving

1198
00:56:52,539 --> 00:56:55,980
from this kind of look aside caching

1199
00:56:55,980 --> 00:56:59,849
strategies is called the thundering herd

1200
00:56:59,849 --> 00:57:06,490
and the the scenario

1201
00:57:06,490 --> 00:57:09,280
is that supposing we have some piece of

1202
00:57:09,280 --> 00:57:12,310
data there's lotsa memcache servers but

1203
00:57:12,310 --> 00:57:13,930
there's some piece of data stored on

1204
00:57:13,930 --> 00:57:16,600
this memcache server there's a whole

1205
00:57:16,600 --> 00:57:20,310
bunch of front ends that are ordinarily

1206
00:57:20,310 --> 00:57:23,560
reading that one piece of very popular

1207
00:57:23,560 --> 00:57:25,900
data so they're all sending constantly

1208
00:57:25,900 --> 00:57:27,369
sending get requests for that data the

1209
00:57:27,369 --> 00:57:29,470
memcache server has it in the cache it

1210
00:57:29,470 --> 00:57:31,450
answers them and you know their memcache

1211
00:57:31,450 --> 00:57:33,820
server is conserve like millions to

1212
00:57:33,820 --> 00:57:36,100
million requests per second so we're

1213
00:57:36,100 --> 00:57:39,250
doing pretty good and of course there's

1214
00:57:39,250 --> 00:57:40,540
some database server sitting back here

1215
00:57:40,540 --> 00:57:42,220
that has the real copy of that data but

1216
00:57:42,220 --> 00:57:43,420
we're not bothering it because it is

1217
00:57:43,420 --> 00:57:46,090
cached well suppose some front-end comes

1218
00:57:46,090 --> 00:57:49,570
along and modifies this very popular

1219
00:57:49,570 --> 00:57:51,010
data so it's going to send a write to

1220
00:57:51,010 --> 00:57:53,080
the database with the new data and then

1221
00:57:53,080 --> 00:57:57,400
it's gonna send a delete to the memcache

1222
00:57:57,400 --> 00:57:58,570
server because that's the way rights

1223
00:57:58,570 --> 00:58:00,580
work so now we've just deleted this

1224
00:58:00,580 --> 00:58:02,800
extremely popular data we have all these

1225
00:58:02,800 --> 00:58:04,600
front ends constantly sending gets for

1226
00:58:04,600 --> 00:58:08,320
that data they're all gonna miss all at

1227
00:58:08,320 --> 00:58:13,660
the same time they're all gonna now

1228
00:58:13,660 --> 00:58:17,410
having missed send a read request to the

1229
00:58:17,410 --> 00:58:19,750
front end database all at the same time

1230
00:58:19,750 --> 00:58:22,090
and so now this front-end database is

1231
00:58:22,090 --> 00:58:23,830
faced with maybe dozens or hundreds of

1232
00:58:23,830 --> 00:58:25,660
simultaneous requests for this data so

1233
00:58:25,660 --> 00:58:27,160
the Loews here is gonna be pretty high

1234
00:58:27,160 --> 00:58:30,580
and it's particularly disappointing

1235
00:58:30,580 --> 00:58:33,609
because we know that all these requests

1236
00:58:33,609 --> 00:58:35,380
are for the same key so the database is

1237
00:58:35,380 --> 00:58:36,790
going to do the same work over and over

1238
00:58:36,790 --> 00:58:39,190
again to respond with the latest written

1239
00:58:39,190 --> 00:58:46,180
copy of that key until finally the front

1240
00:58:46,180 --> 00:58:48,460
ends get around to installing the new

1241
00:58:48,460 --> 00:58:50,619
key in memcache and then people start

1242
00:58:50,619 --> 00:58:53,109
hitting again and so this is the

1243
00:58:53,109 --> 00:58:55,119
Thundering hurt what we'd really like is

1244
00:58:55,119 --> 00:58:58,810
a single you know if a miss if there's a

1245
00:58:58,810 --> 00:59:01,270
right and the leads and a miss happens

1246
00:59:01,270 --> 00:59:03,550
in memcache we'd like what we'd like is

1247
00:59:03,550 --> 00:59:05,590
the for the first front end that misses

1248
00:59:05,590 --> 00:59:07,390
to fetch the data and install it and for

1249
00:59:07,390 --> 00:59:09,400
the other front ends just like take a

1250
00:59:09,400 --> 00:59:11,710
deep breath then wait until the new data

1251
00:59:11,710 --> 00:59:15,380
is cached and that's

1252
00:59:15,380 --> 00:59:17,029
just what their design does if you look

1253
00:59:17,029 --> 00:59:21,109
at the if this thing called Elise which

1254
00:59:21,109 --> 00:59:26,119
is different from the pieces were used

1255
00:59:26,119 --> 00:59:30,049
to but they call Elise and we start from

1256
00:59:30,049 --> 00:59:33,339
scratch in the scenario again let's see

1257
00:59:33,339 --> 00:59:35,539
so now suppose we have a popular piece

1258
00:59:35,539 --> 00:59:40,640
of data the first front end that asks

1259
00:59:40,640 --> 00:59:44,779
for a data that's missing memcache Devo

1260
00:59:44,779 --> 00:59:46,039
will send back an error saying oh no I

1261
00:59:46,039 --> 00:59:47,660
don't have the data in my cache but it

1262
00:59:47,660 --> 00:59:51,529
will install Elise which is a bit unique

1263
00:59:51,529 --> 00:59:55,069
number it'll pick a least number install

1264
00:59:55,069 --> 00:59:57,470
it in a table and the send this lease

1265
00:59:57,470 --> 01:00:01,250
token back to the front end and then

1266
01:00:01,250 --> 01:00:03,079
other front ends that come in and ask

1267
01:00:03,079 --> 01:00:05,890
for the same Keith they'll simply get a

1268
01:00:05,890 --> 01:00:10,849
just be asked to wait you know a quarter

1269
01:00:10,849 --> 01:00:12,259
of a second or whatever some reasonable

1270
01:00:12,259 --> 01:00:13,789
amount of time by the memcache D because

1271
01:00:13,789 --> 01:00:15,230
the memcache key will see a haha I've

1272
01:00:15,230 --> 01:00:16,880
already issued the lease for that key

1273
01:00:16,880 --> 01:00:18,920
now there's at least potentially a V

1274
01:00:18,920 --> 01:00:21,529
Sparky the server will notice it's

1275
01:00:21,529 --> 01:00:22,940
already issued at least for the can tell

1276
01:00:22,940 --> 01:00:26,259
these ones to wait so only one of the

1277
01:00:26,259 --> 01:00:28,789
server's guess Elise this server then

1278
01:00:28,789 --> 01:00:33,230
asks for the data from the database when

1279
01:00:33,230 --> 01:00:35,180
against the responds back mom

1280
01:00:35,180 --> 01:00:39,730
then it sends the put for the new data

1281
01:00:40,210 --> 01:00:43,359
with a key and the value of God and the

1282
01:00:43,359 --> 01:00:45,039
least proved that it was the one who was

1283
01:00:45,039 --> 01:00:46,779
allowed to write the data memcache

1284
01:00:46,779 --> 01:00:48,190
people looking for these today aha yeah

1285
01:00:48,190 --> 01:00:51,640
you are the the person whose lease was

1286
01:00:51,640 --> 01:00:52,960
granted and it'll actually do the

1287
01:00:52,960 --> 01:00:55,690
install by and by these other friends

1288
01:00:55,690 --> 01:00:57,250
who are told the wait will reissue their

1289
01:00:57,250 --> 01:01:00,220
reads now that it will be there and so

1290
01:01:00,220 --> 01:01:03,430
we all if all goes well get just one

1291
01:01:03,430 --> 01:01:05,289
request to the database instead of

1292
01:01:05,289 --> 01:01:08,740
dozens or hundreds and I think it's the

1293
01:01:08,740 --> 01:01:10,089
sense in which is the lease is if the

1294
01:01:10,089 --> 01:01:14,260
front-end fails at an awkward moment and

1295
01:01:14,260 --> 01:01:16,000
doesn't actually request the data from

1296
01:01:16,000 --> 01:01:17,650
the database or doesn't get around it

1297
01:01:17,650 --> 01:01:19,569
installing it memcache D eventually

1298
01:01:19,569 --> 01:01:21,700
memcache D will delete the lease cuz it

1299
01:01:21,700 --> 01:01:23,680
times out and the next front end to ask

1300
01:01:23,680 --> 01:01:26,770
will get a new lease and will hope that

1301
01:01:26,770 --> 01:01:28,299
it will talk to the database and install

1302
01:01:28,299 --> 01:01:32,319
new data so yes they answer the question

1303
01:01:32,319 --> 01:01:35,710
the lease does up a time out in case the

1304
01:01:35,710 --> 01:01:41,140
first front end fails yes yes okay so

1305
01:01:41,140 --> 01:01:43,029
these leases are the their solution to

1306
01:01:43,029 --> 01:01:48,130
the Thundering Herd problem um another

1307
01:01:48,130 --> 01:01:49,930
problem they have is that if one of

1308
01:01:49,930 --> 01:01:54,430
these memcache servers fails the most

1309
01:01:54,430 --> 01:01:56,349
natural you-know-whats if they don't do

1310
01:01:56,349 --> 01:01:58,059
anything special if the memcache server

1311
01:01:58,059 --> 01:02:00,309
fails the front ends will send a request

1312
01:02:00,309 --> 01:02:02,470
they'll get back a timeout and network

1313
01:02:02,470 --> 01:02:04,150
will say jeez that you know I couldn't

1314
01:02:04,150 --> 01:02:05,819
contact that host never got a response

1315
01:02:05,819 --> 01:02:09,039
and what the real I BRE

1316
01:02:09,039 --> 01:02:10,869
software does is it then sense it

1317
01:02:10,869 --> 01:02:13,690
requests the database so if a memcache

1318
01:02:13,690 --> 01:02:14,980
server fails and we don't do anything

1319
01:02:14,980 --> 01:02:17,260
special the database is now going to be

1320
01:02:17,260 --> 01:02:19,720
exposed directly to the reads all of

1321
01:02:19,720 --> 01:02:21,010
these reefs and I'm catch server West

1322
01:02:21,010 --> 01:02:22,779
serving this is the memcache server may

1323
01:02:22,779 --> 01:02:23,859
well have been serving you know a

1324
01:02:23,859 --> 01:02:26,770
million reads per second that may mean

1325
01:02:26,770 --> 01:02:29,020
that the database server would be then

1326
01:02:29,020 --> 01:02:30,339
exposed to those million reads per

1327
01:02:30,339 --> 01:02:31,630
second then it's nowhere near fast

1328
01:02:31,630 --> 01:02:37,440
enough to deal with all those weeds now

1329
01:02:37,440 --> 01:02:39,609
Facebook they don't really talk about in

1330
01:02:39,609 --> 01:02:40,960
the paper but they do have automated

1331
01:02:40,960 --> 01:02:44,289
machinery to replace a failed memcache

1332
01:02:44,289 --> 01:02:48,010
server but that takes a while to sort of

1333
01:02:48,010 --> 01:02:51,549
set up a new server a new memcache

1334
01:02:51,549 --> 01:02:53,850
server and redirect all the front-end

1335
01:02:53,850 --> 01:02:55,380
to the new server instead of the old

1336
01:02:55,380 --> 01:02:57,150
server so in the meantime they need a

1337
01:02:57,150 --> 01:03:00,180
sort of temporary solution and that's

1338
01:03:00,180 --> 01:03:06,510
this gutter idea so let's say the scoop

1339
01:03:06,510 --> 01:03:11,880
is that we have our front ends we have

1340
01:03:11,880 --> 01:03:13,890
the sort of ordinary set of memcache

1341
01:03:13,890 --> 01:03:16,610
servers

1342
01:03:17,610 --> 01:03:21,010
the database the one of the memcache

1343
01:03:21,010 --> 01:03:23,500
service has failed we're kind of waiting

1344
01:03:23,500 --> 01:03:26,530
until the automatic memcache server

1345
01:03:26,530 --> 01:03:27,970
replacement system replaces this

1346
01:03:27,970 --> 01:03:30,780
memcache server in the meantime

1347
01:03:30,780 --> 01:03:32,980
friends are sending requests to it they

1348
01:03:32,980 --> 01:03:35,470
get a sort of server did not respond

1349
01:03:35,470 --> 01:03:38,800
error from the network and then there's

1350
01:03:38,800 --> 01:03:45,540
a presumably small set of gutter servers

1351
01:03:46,109 --> 01:03:50,920
whose only purpose in life is to eye

1352
01:03:50,920 --> 01:03:55,960
they must be idle except when a real

1353
01:03:55,960 --> 01:03:57,579
memcache server fails and when the front

1354
01:03:57,579 --> 01:04:00,190
end gets an error back saying that get

1355
01:04:00,190 --> 01:04:01,810
couldn't contact the memcache server

1356
01:04:01,810 --> 01:04:05,140
it'll send the same request to one of

1357
01:04:05,140 --> 01:04:06,339
the gutter servers and though the paper

1358
01:04:06,339 --> 01:04:08,349
doesn't say I imagine that the front end

1359
01:04:08,349 --> 01:04:10,210
will again hash the key in order to

1360
01:04:10,210 --> 01:04:13,650
choose which gutter server to talk to

1361
01:04:13,800 --> 01:04:17,589
and if the gutter server has the value

1362
01:04:17,589 --> 01:04:18,819
that's great

1363
01:04:18,819 --> 01:04:21,040
otherwise the front end server will

1364
01:04:21,040 --> 01:04:22,630
contact the database server to read the

1365
01:04:22,630 --> 01:04:25,930
value and then install it in the

1366
01:04:25,930 --> 01:04:27,339
memcache server in case somebody else

1367
01:04:27,339 --> 01:04:28,750
answer asks for the same data

1368
01:04:28,750 --> 01:04:32,410
so while this means down the gutter

1369
01:04:32,410 --> 01:04:38,290
servers will handle basically handle its

1370
01:04:38,290 --> 01:04:39,910
request and so they'll be a miss you

1371
01:04:39,910 --> 01:04:41,740
know handled by lease leases the

1372
01:04:41,740 --> 01:04:42,579
Thundering Herd

1373
01:04:42,579 --> 01:04:44,859
they'll be at least I need a Miss on

1374
01:04:44,859 --> 01:04:46,810
each of the items that was a no-fail

1375
01:04:46,810 --> 01:04:49,060
memcache server so there will be some

1376
01:04:49,060 --> 01:04:50,319
load in the database server but then

1377
01:04:50,319 --> 01:04:51,730
hopefully quickly this memcache server

1378
01:04:51,730 --> 01:04:54,460
will I get all the data that's listen

1379
01:04:54,460 --> 01:05:00,280
use and provide good service and then by

1380
01:05:00,280 --> 01:05:01,540
and by this will be replaced and then

1381
01:05:01,540 --> 01:05:03,339
the friends will know to talk to a

1382
01:05:03,339 --> 01:05:07,180
different replacement server and because

1383
01:05:07,180 --> 01:05:09,099
they don't and this is today's question

1384
01:05:09,099 --> 01:05:13,150
I think that they don't send deletes to

1385
01:05:13,150 --> 01:05:14,980
these gutter servers because since a

1386
01:05:14,980 --> 01:05:16,569
gutter server could have taken over for

1387
01:05:16,569 --> 01:05:20,740
anyone and maybe more than one of the

1388
01:05:20,740 --> 01:05:22,119
ordinary memcache service it could

1389
01:05:22,119 --> 01:05:26,760
actually have cache the caching any key

1390
01:05:26,760 --> 01:05:30,910
so that would mean that and there may be

1391
01:05:30,910 --> 01:05:33,400
you know friends talking to it that

1392
01:05:33,400 --> 01:05:35,290
would mean that whenever a front-end

1393
01:05:35,290 --> 01:05:36,730
needs it to delete a key from memcache

1394
01:05:36,730 --> 01:05:40,569
or when the squeal on the database sends

1395
01:05:40,569 --> 01:05:42,520
a delete for any key to the relevant

1396
01:05:42,520 --> 01:05:46,420
memcache server yeah you know the the

1397
01:05:46,420 --> 01:05:47,740
natural design would be that it would

1398
01:05:47,740 --> 01:05:51,609
also send a copy of that delete to every

1399
01:05:51,609 --> 01:05:53,800
one of the gutter servers and the same

1400
01:05:53,800 --> 01:05:55,119
for front ends that are deleting data

1401
01:05:55,119 --> 01:05:56,619
they would delete from the memcache

1402
01:05:56,619 --> 01:05:58,180
stores but they would also have to leave

1403
01:05:58,180 --> 01:05:59,989
potentially from any

1404
01:05:59,989 --> 01:06:04,289
MCAD gutter server that would double the

1405
01:06:04,289 --> 01:06:05,460
amount of defeats that had to be sent

1406
01:06:05,460 --> 01:06:06,839
around even though most of the time

1407
01:06:06,839 --> 01:06:08,220
these gutter servers aren't doing

1408
01:06:08,220 --> 01:06:09,690
anything and don't cache anything and it

1409
01:06:09,690 --> 01:06:11,849
doesn't matter and so in order to avoid

1410
01:06:11,849 --> 01:06:15,619
all these extra deletes they actually

1411
01:06:15,619 --> 01:06:18,299
fix the gutter servers so that they

1412
01:06:18,299 --> 01:06:22,619
delete Keys very rapidly instead of

1413
01:06:22,619 --> 01:06:23,940
hanging on to them until they're

1414
01:06:23,940 --> 01:06:27,989
explicitly deleted that was answer to

1415
01:06:27,989 --> 01:06:34,349
the question all right so I wanna talk a

1416
01:06:34,349 --> 01:06:38,999
bit about consistency all this at a

1417
01:06:38,999 --> 01:06:43,160
super high level you know the

1418
01:06:43,160 --> 01:06:45,210
consistency problem is that there's lots

1419
01:06:45,210 --> 01:06:47,579
of copies of the data for any given

1420
01:06:47,579 --> 01:06:50,519
piece of data you know there's a copy in

1421
01:06:50,519 --> 01:06:53,069
the primary database there's a copy in

1422
01:06:53,069 --> 01:06:54,749
the corresponding database server of

1423
01:06:54,749 --> 01:06:58,049
each of the secondary regions there's a

1424
01:06:58,049 --> 01:07:01,799
copy of that key in each local cluster

1425
01:07:01,799 --> 01:07:03,930
in one of the memcache keys in each

1426
01:07:03,930 --> 01:07:06,359
local cluster there may be copies of

1427
01:07:06,359 --> 01:07:08,700
that key and the gutter servers and

1428
01:07:08,700 --> 01:07:11,400
there may be copies of the key in the

1429
01:07:11,400 --> 01:07:13,170
memcache servers and the gutter memcache

1430
01:07:13,170 --> 01:07:14,849
servers at each other region so we have

1431
01:07:14,849 --> 01:07:17,400
lots and lots of copies of every piece

1432
01:07:17,400 --> 01:07:19,259
of data running around when a write

1433
01:07:19,259 --> 01:07:22,319
comes in you know the stuff has to

1434
01:07:22,319 --> 01:07:23,930
happen on all those copies and

1435
01:07:23,930 --> 01:07:26,369
furthermore the writes may come from

1436
01:07:26,369 --> 01:07:28,319
multiple sources the same key may be

1437
01:07:28,319 --> 01:07:30,210
written at the same time by multiple

1438
01:07:30,210 --> 01:07:32,009
front ends and this region may be by

1439
01:07:32,009 --> 01:07:35,910
friends and other regions too and so

1440
01:07:35,910 --> 01:07:38,249
it's this concurrency and multiple

1441
01:07:38,249 --> 01:07:40,499
copies and sort of multiple sources of

1442
01:07:40,499 --> 01:07:42,859
writes since there's multiple front ends

1443
01:07:42,859 --> 01:07:47,039
it creates a lot of opportunity for not

1444
01:07:47,039 --> 01:07:49,440
just for there to be stale data but for

1445
01:07:49,440 --> 01:07:52,259
data stale data to be left in the system

1446
01:07:52,259 --> 01:07:58,710
for long periods of time and so I want

1447
01:07:58,710 --> 01:08:02,160
to I want to illustrate what are those

1448
01:08:02,160 --> 01:08:03,450
problems actually in a sense we've

1449
01:08:03,450 --> 01:08:05,219
already talked a bit about this when

1450
01:08:05,219 --> 01:08:08,700
somebody asked why the front ends don't

1451
01:08:08,700 --> 01:08:10,499
update why do they delete instead of

1452
01:08:10,499 --> 01:08:12,089
updating so that's certainly one

1453
01:08:12,089 --> 01:08:13,570
instance of the kind of weather

1454
01:08:13,570 --> 01:08:17,050
multiple sources of data and so we have

1455
01:08:17,050 --> 01:08:22,060
trouble enforcing correct order but

1456
01:08:22,060 --> 01:08:25,830
here's another example of a race an

1457
01:08:25,830 --> 01:08:28,450
update race that if they hadn't done

1458
01:08:28,450 --> 01:08:30,420
something about it would have left data

1459
01:08:30,420 --> 01:08:31,779
indefinitely

1460
01:08:31,779 --> 01:08:35,369
stale data and definitely in memcache

1461
01:08:36,540 --> 01:08:38,710
it's going to be a similar flavor to the

1462
01:08:38,710 --> 01:08:40,540
previous example so supposing we have

1463
01:08:40,540 --> 01:08:47,430
client one he wants to read a key but

1464
01:08:47,430 --> 01:08:51,760
memcache says it doesn't have the data

1465
01:08:51,760 --> 01:08:55,870
it's a Miss so C one's gonna read the

1466
01:08:55,870 --> 01:09:02,350
data from from the database and let's

1467
01:09:02,350 --> 01:09:07,750
say it gets back some value that you

1468
01:09:07,750 --> 01:09:09,590
want

1469
01:09:09,590 --> 01:09:12,990
meanwhile client to wants to update this

1470
01:09:12,990 --> 01:09:18,510
data so it sends you know its rates he

1471
01:09:18,510 --> 01:09:22,880
equals v2 and sends that to the database

1472
01:09:22,880 --> 01:09:26,670
and then you know the rule for writes

1473
01:09:26,670 --> 01:09:28,170
the code for writes that we saw is that

1474
01:09:28,170 --> 01:09:29,729
the next thing we do is delete it from

1475
01:09:29,729 --> 01:09:33,660
the database from memcache d.c c2 is

1476
01:09:33,660 --> 01:09:35,390
going to delete

1477
01:09:35,390 --> 01:09:39,439
ah the key from the database oh that's a

1478
01:09:39,439 --> 01:09:41,450
Friday you know it was actually c2

1479
01:09:41,450 --> 01:09:42,979
doesn't really know what's in memcache d

1480
01:09:42,979 --> 01:09:44,359
but the leading was ever there is always

1481
01:09:44,359 --> 01:09:48,430
safe because certainly not gonna cause a

1482
01:09:48,430 --> 01:09:50,990
stale data to be deleting won't cause

1483
01:09:50,990 --> 01:09:53,359
her to be stilled Leena um and this is

1484
01:09:53,359 --> 01:09:55,310
the sense that the paper claims that

1485
01:09:55,310 --> 01:09:59,210
delete is idempotent said delete it's

1486
01:09:59,210 --> 01:10:02,810
always safe to kabhi but if you recall

1487
01:10:02,810 --> 01:10:05,960
the pseudocode for what a read does if

1488
01:10:05,960 --> 01:10:07,550
you miss and you read the data from the

1489
01:10:07,550 --> 01:10:09,140
database you're supposed to just insert

1490
01:10:09,140 --> 01:10:12,140
that data into memcache so client 1 you

1491
01:10:12,140 --> 01:10:13,850
know may have been slow and finally gets

1492
01:10:13,850 --> 01:10:19,430
around to sending a set RPC two memcache

1493
01:10:19,430 --> 01:10:21,710
T but it read version 1 and read a you

1494
01:10:21,710 --> 01:10:23,870
know what is now an old outdated version

1495
01:10:23,870 --> 01:10:26,600
of the data from the database but it's

1496
01:10:26,600 --> 01:10:29,620
going to set that into

1497
01:10:33,020 --> 01:10:35,660
set this into memcache and yeah you know

1498
01:10:35,660 --> 01:10:36,980
one other thing that happened is that we

1499
01:10:36,980 --> 01:10:39,170
know the database is is whenever you

1500
01:10:39,170 --> 01:10:40,730
write something I'm database that sends

1501
01:10:40,730 --> 01:10:42,830
deletes to memcache D so of course maybe

1502
01:10:42,830 --> 01:10:44,270
at this point the database will also

1503
01:10:44,270 --> 01:10:47,510
have sent a delete

1504
01:10:47,510 --> 01:10:51,060
for k2m caste and so now we get to

1505
01:10:51,060 --> 01:10:52,080
deletes but it doesn't really matter

1506
01:10:52,080 --> 01:10:53,160
right these lease may already have

1507
01:10:53,160 --> 01:10:55,200
happened by the time client one gets

1508
01:10:55,200 --> 01:10:59,730
around to updating this key and so at

1509
01:10:59,730 --> 01:11:03,480
this point indefinitely the memcache D

1510
01:11:03,480 --> 01:11:06,420
will be cashing a stale version of of

1511
01:11:06,420 --> 01:11:08,040
this data and there's just no mechanism

1512
01:11:08,040 --> 01:11:11,820
anymore or this is them if the system

1513
01:11:11,820 --> 01:11:14,100
worked in just this way there's no

1514
01:11:14,100 --> 01:11:16,560
mechanism for the memcache D to ever see

1515
01:11:16,560 --> 01:11:20,310
to ever get the actual correct value

1516
01:11:20,310 --> 01:11:23,670
it's gonna store and serve up stale data

1517
01:11:23,670 --> 01:11:27,230
for key K forever

1518
01:11:27,369 --> 01:11:30,219
and they because they ran into this and

1519
01:11:30,219 --> 01:11:32,690
while they're okay with data being

1520
01:11:32,690 --> 01:11:35,090
somewhat out-of-date they're not okay

1521
01:11:35,090 --> 01:11:37,250
with data being out of date forever

1522
01:11:37,250 --> 01:11:38,989
because users will eventually notice

1523
01:11:38,989 --> 01:11:42,349
that they're seeing ancient data and so

1524
01:11:42,349 --> 01:11:44,270
they had to solve this they had to make

1525
01:11:44,270 --> 01:11:49,239
sure that this scenario didn't happen

1526
01:11:49,250 --> 01:11:51,349
they actually solved this this problem

1527
01:11:51,349 --> 01:11:54,920
also with the lease mechanism at the

1528
01:11:54,920 --> 01:11:56,300
same lease mechanism that we describe

1529
01:11:56,300 --> 01:11:58,550
for the Thundering hoard although

1530
01:11:58,550 --> 01:12:01,460
there's an extension to the lease

1531
01:12:01,460 --> 01:12:04,159
mechanism that makes this work so what

1532
01:12:04,159 --> 01:12:06,469
happens is that when memcache descends

1533
01:12:06,469 --> 01:12:07,940
back a Miss indication seeing the data

1534
01:12:07,940 --> 01:12:10,010
wasn't in the cache it's gonna grant the

1535
01:12:10,010 --> 01:12:13,099
lease so we get the Miss indication plus

1536
01:12:13,099 --> 01:12:17,329
this lease which is basically just a big

1537
01:12:17,329 --> 01:12:19,460
unique number and the memcache server is

1538
01:12:19,460 --> 01:12:21,590
gonna remember that the association

1539
01:12:21,590 --> 01:12:23,840
between this lease and this key it knows

1540
01:12:23,840 --> 01:12:27,440
that somebody out there with a lease to

1541
01:12:27,440 --> 01:12:31,340
update this key the new rule is that

1542
01:12:31,340 --> 01:12:34,699
when the when the memcache server gets a

1543
01:12:34,699 --> 01:12:36,800
delete from either another client or

1544
01:12:36,800 --> 01:12:39,679
from the database server the memcache

1545
01:12:39,679 --> 01:12:42,440
server is gonna as well as deleting the

1546
01:12:42,440 --> 01:12:44,420
item is going to invalidate this lease

1547
01:12:44,420 --> 01:12:46,849
so as soon as either these deletes come

1548
01:12:46,849 --> 01:12:49,579
in assuming that Elyse arrived first the

1549
01:12:49,579 --> 01:12:52,460
memcache server is gonna believe this

1550
01:12:52,460 --> 01:12:55,250
lease from its table about leases this

1551
01:12:55,250 --> 01:12:57,610
set

1552
01:12:59,679 --> 01:13:02,739
is the lease back from the front end now

1553
01:13:02,739 --> 01:13:05,739
when the set arrives the memcache server

1554
01:13:05,739 --> 01:13:06,969
will look at the lease and say wait a

1555
01:13:06,969 --> 01:13:09,760
minute I you don't have a lease for this

1556
01:13:09,760 --> 01:13:12,280
key all right invalid if these fit this

1557
01:13:12,280 --> 01:13:14,980
key I'm gonna ignore this set so because

1558
01:13:14,980 --> 01:13:16,630
the lease has been because one of these

1559
01:13:16,630 --> 01:13:18,969
if one of these deletes came in before

1560
01:13:18,969 --> 01:13:21,699
the set this sees to be invalid in

1561
01:13:21,699 --> 01:13:24,040
invalidated and the memcache server

1562
01:13:24,040 --> 01:13:27,130
would ignore this set and that would

1563
01:13:27,130 --> 01:13:29,920
mean that the key would note just stay

1564
01:13:29,920 --> 01:13:33,340
missing from memcache and the next

1565
01:13:33,340 --> 01:13:34,900
client that tried to read that key

1566
01:13:34,900 --> 01:13:37,060
you'll get a Miss would read the fresh

1567
01:13:37,060 --> 01:13:40,120
data now from the database and would

1568
01:13:40,120 --> 01:13:41,560
install it in memcache and presumably

1569
01:13:41,560 --> 01:13:43,900
the second time around

1570
01:13:43,900 --> 01:13:46,920
the second readers lease would be valid

1571
01:13:46,920 --> 01:13:49,900
um you may and indeed you should ask

1572
01:13:49,900 --> 01:13:51,040
what happened that the order is

1573
01:13:51,040 --> 01:13:53,969
different so supposing these deletes

1574
01:13:53,969 --> 01:13:56,429
instead of happening before the set

1575
01:13:56,429 --> 01:13:59,489
these deletes were instead to have

1576
01:13:59,489 --> 01:14:02,409
happen after the set I want to make sure

1577
01:14:02,409 --> 01:14:05,860
this scheme still works then and so how

1578
01:14:05,860 --> 01:14:08,830
things would play out then is that since

1579
01:14:08,830 --> 01:14:12,580
if these Dilys were late happened after

1580
01:14:12,580 --> 01:14:14,739
the set the memcache server wouldn't

1581
01:14:14,739 --> 01:14:17,100
delete these from its table of visas

1582
01:14:17,100 --> 01:14:19,360
Solis would still be there when the set

1583
01:14:19,360 --> 01:14:22,150
came and yes indeed we would still then

1584
01:14:22,150 --> 01:14:25,630
it would accept the setting we would be

1585
01:14:25,630 --> 01:14:30,040
setting key to a stale value but our

1586
01:14:30,040 --> 01:14:31,510
assumption was this time that the

1587
01:14:31,510 --> 01:14:33,250
deletes had been late and that means the

1588
01:14:33,250 --> 01:14:35,260
Dilys are yet to arrive and when they

1589
01:14:35,260 --> 01:14:37,300
when these deletes arrive then this stay

1590
01:14:37,300 --> 01:14:39,489
on theta will be knocked out of the

1591
01:14:39,489 --> 01:14:41,380
cache and so the stale date will be in

1592
01:14:41,380 --> 01:14:43,000
the cache a little bit longer but we

1593
01:14:43,000 --> 01:14:45,760
won't have this situation where stale

1594
01:14:45,760 --> 01:14:47,610
data is sitting in the cache

1595
01:14:47,610 --> 01:14:51,390
indefinitely and never deleted

1596
01:14:52,260 --> 01:14:54,890
any questions

1597
01:14:54,890 --> 01:15:07,910
lissa machinery okay um to wrap up you

1598
01:15:07,910 --> 01:15:10,950
it's certainly fair to view this system

1599
01:15:10,950 --> 01:15:12,810
a lot of the complexity of the system as

1600
01:15:12,810 --> 01:15:14,910
stemming from the fact that it was sort

1601
01:15:14,910 --> 01:15:17,330
of put together out of pieces that

1602
01:15:17,330 --> 01:15:20,010
didn't know about each other like it

1603
01:15:20,010 --> 01:15:21,900
would be nice for example memcached he

1604
01:15:21,900 --> 01:15:24,060
knew about the database I'm understand

1605
01:15:24,060 --> 01:15:25,890
memcache D and the database kind of

1606
01:15:25,890 --> 01:15:31,880
cooperated in a consistency scheme and

1607
01:15:31,880 --> 01:15:35,940
perhaps if Facebook could have at the

1608
01:15:35,940 --> 01:15:38,360
very beginning you know predicted the

1609
01:15:38,360 --> 01:15:40,440
how things would play out on what the

1610
01:15:40,440 --> 01:15:42,410
problems would be and if they have had

1611
01:15:42,410 --> 01:15:44,490
enough engineers to work on it they

1612
01:15:44,490 --> 01:15:45,600
could have from the beginning built a

1613
01:15:45,600 --> 01:15:48,480
system that could provide both all the

1614
01:15:48,480 --> 01:15:50,120
things they needed high-performance

1615
01:15:50,120 --> 01:15:54,120
multi data center replication partition

1616
01:15:54,120 --> 01:15:57,620
and everything and they're having

1617
01:15:57,620 --> 01:15:59,580
companies that have done that so the

1618
01:15:59,580 --> 01:16:01,710
example I know of that sort of most

1619
01:16:01,710 --> 01:16:06,420
directly comparable to the system in

1620
01:16:06,420 --> 01:16:08,850
this paper is that if you care about

1621
01:16:08,850 --> 01:16:10,800
this stuff you might want to look at it

1622
01:16:10,800 --> 01:16:16,820
is Yahoo's peanuts storage system which

1623
01:16:16,820 --> 01:16:20,540
in a sort of designed from scratch and

1624
01:16:20,540 --> 01:16:23,100
you know different different in many

1625
01:16:23,100 --> 01:16:26,100
details but it does provide multi-site

1626
01:16:26,100 --> 01:16:30,150
replication with consistency and good

1627
01:16:30,150 --> 01:16:32,730
performance so it's possible to do

1628
01:16:32,730 --> 01:16:34,850
better but you know all the issues are

1629
01:16:34,850 --> 01:16:37,130
present that's just had a more

1630
01:16:37,130 --> 01:16:40,320
integrated perhaps elegant set of

1631
01:16:40,320 --> 01:16:41,270
solutions

1632
01:16:41,270 --> 01:16:44,810
the takeaway so for us from this paper

1633
01:16:44,810 --> 01:16:47,970
one is that for them at least and for

1634
01:16:47,970 --> 01:16:50,310
many big operations caching is vital

1635
01:16:50,310 --> 01:16:54,210
absolutely vital for to survive high

1636
01:16:54,210 --> 01:16:57,060
load and the caching is not so much

1637
01:16:57,060 --> 01:16:59,580
about reducing latency it's much more

1638
01:16:59,580 --> 01:17:02,750
about hiding enormous load from

1639
01:17:02,750 --> 01:17:05,570
relatively slow

1640
01:17:05,570 --> 01:17:08,880
storage servers that's what a cache is

1641
01:17:08,880 --> 01:17:11,519
really doing for Facebook is hiding sort

1642
01:17:11,519 --> 01:17:13,260
of concealing almost all the load from

1643
01:17:13,260 --> 01:17:18,059
the database servers another takeaway is

1644
01:17:18,059 --> 01:17:20,730
that you always in big systems you

1645
01:17:20,730 --> 01:17:23,130
always need to be thinking about caching

1646
01:17:23,130 --> 01:17:25,889
versus control versus sorry partition

1647
01:17:25,889 --> 01:17:28,409
versus replication I mean you need ways

1648
01:17:28,409 --> 01:17:32,249
of either formally or informally sort of

1649
01:17:32,249 --> 01:17:34,019
deciding how much your resources are

1650
01:17:34,019 --> 01:17:35,639
going to be devoted to partitioning and

1651
01:17:35,639 --> 01:17:40,579
how much to replication and finally

1652
01:17:40,579 --> 01:17:42,780
ideally you'd be able to do a better job

1653
01:17:42,780 --> 01:17:45,239
in this paper about from the beginning

1654
01:17:45,239 --> 01:17:47,219
integrating the different storage layers

1655
01:17:47,219 --> 01:17:51,469
in order to achieve good consistency

1656
01:17:51,469 --> 01:17:56,219
okay that is all I have to say please

1657
01:17:56,219 --> 01:17:59,510
ask me questions if you have

